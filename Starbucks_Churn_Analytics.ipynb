{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9VF+lwI8Da2dlFsxwh3fe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Starbucks Next Best Action (NBA) System\n",
        "\n",
        "## What This Project Does\n",
        "\n",
        "Imagine running a coffee shop with thousands of customers. You want to send promotional offers (discounts, BOGO deals), but you face three challenges:\n",
        "\n",
        "1. **Who should get an offer?** Not everyone needs one\n",
        "2. **Which offer should each person get?** Different people like different things\n",
        "3. **Will it make money?** Offers cost money - we need profit, not just sales\n",
        "\n",
        "**This AI system answers all three questions automatically for each customer.**\n",
        "\n",
        "---\n",
        "\n",
        "## The Solution\n",
        "\n",
        "Instead of sending the same coupon to everyone (wasteful!), this system:\n",
        "- Identifies which customers actually need an incentive\n",
        "- Picks the perfect offer type for each person\n",
        "- Calculates if we'll make profit before sending anything\n",
        "- Explains WHY each recommendation was made\n",
        "\n",
        "**Result:** Send the right offer to the right person, or save money by not sending anything when it won't help.\n",
        "\n",
        "---\n",
        "\n",
        "## What We Analyzed\n",
        "\n",
        "- **17,000 customers**\n",
        "- **138,000+ purchases**\n",
        "- **76,000+ promotional offers** sent over time\n",
        "- Customer info: age, income, shopping habits, email engagement\n",
        "\n",
        "Think of it as: Years of Starbucks receipts, email campaigns, and customer profiles all in one place.\n",
        "\n",
        "---\n",
        "\n",
        "## How It Works (5 Steps)\n",
        "\n",
        "### Step 1: Clean The Data\n",
        "**What:** Organized messy customer records into usable information  \n",
        "**Why:** Can't make smart decisions with messy data  \n",
        "**Like:** Organizing a messy filing cabinet before doing your taxes\n",
        "\n",
        "### Step 2: Create Smart Metrics\n",
        "**What:** Built \"scores\" that predict customer behavior  \n",
        "**Metrics created:**\n",
        "- **Recency** - How recently did they shop? (newer = better)\n",
        "- **Frequency** - How often do they visit? (more = better)  \n",
        "- **Monetary** - How much do they spend? (higher = better)\n",
        "- **Engagement** - Do they open/use our offers?\n",
        "\n",
        "**Like:** Creating a customer report card with grades for \"Shopping Frequency\" and \"Email Engagement\"\n",
        "\n",
        "### Step 3: Group Customers into Segments\n",
        "**What:** Sorted 17,000 customers into 4 types based on behavior  \n",
        "**Why:** Different customer types need different strategies  \n",
        "\n",
        "**The 4 Groups We Found:**\n",
        "1. **Champions** - Loyal, frequent shoppers (treat them well!)\n",
        "2. **Promising** - Growing customers (encourage them!)\n",
        "3. **At Risk** - Used to shop a lot, now fading (win them back!)\n",
        "4. **Casual** - Occasional visitors (light touch)\n",
        "\n",
        "**Like:** Organizing friends into groups - gym buddies, movie friends, coffee friends. Each group gets different invitations.\n",
        "\n",
        "### Step 4: Predict Who Might Leave (Churn)\n",
        "**What:** AI model predicts which customers will stop visiting  \n",
        "**Accuracy:** 85% correct  \n",
        "\n",
        "**Risk Levels:**\n",
        "- **Low Risk** (56% of customers) - Regular visitors, all good\n",
        "- **Medium Risk** (32% of customers) - Some warning signs\n",
        "- **High Risk** (12% of customers) - Likely to leave soon\n",
        "\n",
        "**Why it matters:** Keeping a customer is easier than winning them back after they leave\n",
        "\n",
        "**Like:** A health check-up - we spot problems before they get serious\n",
        "\n",
        "### Step 5: Recommend Best Action for Each Person\n",
        "**What:** AI picks the best offer (or no offer) for maximum profit  \n",
        "\n",
        "**Options:**\n",
        "- **BOGO** (Buy One Get One) - Costs us $5\n",
        "- **Discount** - Costs us $3\n",
        "- **Informational** (just news) - Costs us $0.50\n",
        "- **No Offer** - Sometimes the best move is doing nothing!\n",
        "\n",
        "**Final Recommendations:**\n",
        "- 30,499 customers â†’ Send BOGO\n",
        "- 30,345 customers â†’ Send Discount  \n",
        "- 15,235 customers â†’ Send Info only\n",
        "- Rest â†’ Don't send anything (they'll buy anyway or won't respond)\n",
        "\n",
        "**Like:** A smart assistant saying \"Customer A loves freebies, send BOGO. Customer B is price-sensitive, send discount. Customer C will buy anyway, don't waste money on them.\"\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Matters\n",
        "\n",
        "**Traditional Approach:**\n",
        "\"Send the same 20% discount to everyone!\"\n",
        "- Wastes money on people who'd buy anyway\n",
        "- Wrong offer for people who want BOGO\n",
        "- Annoys people who don't want emails\n",
        "\n",
        "**Our Smart Approach:**\n",
        "\"Send personalized offers only when they'll actually help\"\n",
        "- Right offer for each person\n",
        "- Only send when profit is positive\n",
        "- Save money by NOT sending to 25% of customers\n",
        "\n",
        "**Impact:**\n",
        "- **Average profit per customer:** $1.67\n",
        "- **Total expected profit:** $127,000\n",
        "- **Customers with positive ROI:** 75% (we skip the other 25%)\n",
        "\n",
        "**Simple math:** Instead of $100,000 in random coupons with $80,000 return, we spend $50,000 in smart coupons with $177,000 return.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## What Makes This Special\n",
        "\n",
        "**Not just a simple coupon system:**\n",
        "\n",
        "1. **Predicts the future** - Who will leave? Who will buy?\n",
        "2. **Personalizes** - Different people get different recommendations\n",
        "3. **Considers profit** - Factors in offer costs, not just revenue\n",
        "4. **Explains itself** - Shows WHY each recommendation was made (not a black box)\n",
        "5. **Production ready** - Can be deployed to a real business today\n",
        "\n",
        "**Most companies just:** \"Send 20% off to everyone on their birthday\"  \n",
        "**This system:** \"Send BOGO to Sarah (she loves those), nothing to John (he'll buy anyway), and a $5 discount to Maria (she's price-sensitive and at risk of leaving)\"\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ“ Technical Skills Demonstrated\n",
        "\n",
        "For technical readers, this project shows:\n",
        "- Data cleaning & engineering (76,000 messy records â†’ clean dataset)\n",
        "- Feature engineering (RFM analysis, engagement metrics)\n",
        "- Unsupervised learning (K-Means clustering for segmentation)\n",
        "- Supervised learning (XGBoost for churn prediction, 85% accuracy)\n",
        "- Causal inference (Uplift modeling with T-Learner)\n",
        "- Business analytics (profit optimization, ROI calculation)\n",
        "- Model explainability (SHAP values)\n",
        "- Production deployment (saved models, API functions)\n",
        "\n"
      ],
      "metadata": {
        "id": "hAep8RX3ePp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (run this first in Colab)\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm shap imbalanced-learn -q\n",
        "\n",
        "print(\" All packages installed successfully!\")"
      ],
      "metadata": {
        "id": "aueD9mRao7Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "\n",
        "# Setup project directory\n",
        "PROJECT_DIR = \"/content/nba_retention\"\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# Clone the repository with the dataset\n",
        "!rm -rf Starbucks-Capstone\n",
        "!git clone --depth 1 https://github.com/reachanihere/Starbucks-Capstone.git\n",
        "\n",
        "# Confirm the files exist\n",
        "print(\"Dataset files:\")\n",
        "!ls -lah Starbucks-Capstone/data\n",
        "\n",
        "print(f\"\\n Working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "l0bGzDhSpUpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Starbucks Customer Segmentation & Next Best Action (NBA) Project\n",
        "=================================================================\n",
        "\"\"\"\n",
        "\n",
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning (we'll use these later)\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "# Visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STARBUCKS CUSTOMER SEGMENTATION & NEXT BEST ACTION PROJECT\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n All libraries imported successfully!\")\n",
        "print(f\" Current date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\" Pandas version: {pd.__version__}\")\n",
        "print(f\" NumPy version: {np.__version__}\")"
      ],
      "metadata": {
        "id": "U1ilrdd8pnoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load the three Starbucks JSON files from the cloned repository\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"LOADING STARBUCKS DATA FILES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define file paths\n",
        "data_dir = 'Starbucks-Capstone/data'\n",
        "\n",
        "# Load Portfolio (Offer information)\n",
        "print(\"\\nLoading portfolio.json...\")\n",
        "portfolio = pd.read_json(f'{data_dir}/portfolio.json', orient='records', lines=True)\n",
        "print(f\"Portfolio loaded: {portfolio.shape[0]} offers Ã— {portfolio.shape[1]} columns\")\n",
        "\n",
        "# Load Profile (Customer demographics)\n",
        "print(\"\\nLoading profile.json...\")\n",
        "profile = pd.read_json(f'{data_dir}/profile.json', orient='records', lines=True)\n",
        "print(f\"Profile loaded: {profile.shape[0]:,} customers Ã— {profile.shape[1]} columns\")\n",
        "\n",
        "# Load Transcript (Event log)\n",
        "print(\"\\n Loading transcript.json...\")\n",
        "transcript = pd.read_json(f'{data_dir}/transcript.json', orient='records', lines=True)\n",
        "print(f\" Transcript loaded: {transcript.shape[0]:,} events Ã— {transcript.shape[1]} columns\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\" ALL DATA FILES LOADED SUCCESSFULLY\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "5m24C_pOpqwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Quick exploration of each dataset\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" INITIAL DATA EXPLORATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" PORTFOLIO (Offer Information)\")\n",
        "print(\"=\"*80)\n",
        "display(portfolio.head())\n",
        "print(f\"\\nShape: {portfolio.shape}\")\n",
        "print(f\"Columns: {portfolio.columns.tolist()}\")\n",
        "print(f\"\\nData types:\\n{portfolio.dtypes}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" PROFILE (Customer Demographics)\")\n",
        "print(\"=\"*80)\n",
        "display(profile.head())\n",
        "print(f\"\\nShape: {profile.shape}\")\n",
        "print(f\"Columns: {profile.columns.tolist()}\")\n",
        "print(f\"\\nData types:\\n{profile.dtypes}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"  TRANSCRIPT (Event Log)\")\n",
        "print(\"=\"*80)\n",
        "display(transcript.head(10))\n",
        "print(f\"\\nShape: {transcript.shape}\")\n",
        "print(f\"Columns: {transcript.columns.tolist()}\")\n",
        "print(f\"\\nData types:\\n{transcript.dtypes}\")"
      ],
      "metadata": {
        "id": "reSgy3Acpu8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Check for missing values and data quality issues\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" DATA QUALITY ASSESSMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n PORTFOLIO - Missing Values:\")\n",
        "print(\"-\" * 40)\n",
        "missing_portfolio = portfolio.isnull().sum()\n",
        "print(missing_portfolio[missing_portfolio > 0] if missing_portfolio.sum() > 0 else \"No missing values âœ“\")\n",
        "\n",
        "print(\"\\n PROFILE - Missing Values:\")\n",
        "print(\"-\" * 40)\n",
        "missing_profile = profile.isnull().sum()\n",
        "print(missing_profile)\n",
        "print(f\"\\nMissing percentage:\")\n",
        "print((missing_profile / len(profile) * 100).round(2))\n",
        "\n",
        "print(\"\\n TRANSCRIPT - Missing Values:\")\n",
        "print(\"-\" * 40)\n",
        "missing_transcript = transcript.isnull().sum()\n",
        "print(missing_transcript[missing_transcript > 0] if missing_transcript.sum() > 0 else \"No missing values âœ“\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\" KEY INSIGHTS:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"â€¢ Portfolio: {len(portfolio)} unique offers\")\n",
        "print(f\"â€¢ Profile: {len(profile):,} customers with {profile['gender'].isnull().sum():,} missing gender\")\n",
        "print(f\"â€¢ Transcript: {len(transcript):,} total events\")\n",
        "print(f\"â€¢ Event types: {transcript['event'].nunique()} unique event types\")"
      ],
      "metadata": {
        "id": "CvHp-s7BpzZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Understand the event types in the transcript\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" EVENT TYPE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n Event Type Distribution:\")\n",
        "print(\"-\" * 40)\n",
        "event_counts = transcript['event'].value_counts()\n",
        "print(event_counts)\n",
        "\n",
        "print(\"\\n Event Type Percentages:\")\n",
        "print(\"-\" * 40)\n",
        "event_pct = (transcript['event'].value_counts(normalize=True) * 100).round(2)\n",
        "print(event_pct)\n",
        "\n",
        "# Visualize event distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "event_counts.plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black')\n",
        "ax1.set_title('Event Type Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Event Type', fontsize=12)\n",
        "ax1.set_ylabel('Count', fontsize=12)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add count labels on bars\n",
        "for i, v in enumerate(event_counts):\n",
        "    ax1.text(i, v + 1000, f'{v:,}', ha='center', fontsize=10)\n",
        "\n",
        "# Percentage plot\n",
        "event_pct.plot(kind='bar', ax=ax2, color='coral', edgecolor='black')\n",
        "ax2.set_title('Event Type Distribution (%)', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Event Type', fontsize=12)\n",
        "ax2.set_ylabel('Percentage (%)', fontsize=12)\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add percentage labels on bars\n",
        "for i, v in enumerate(event_pct):\n",
        "    ax2.text(i, v + 0.5, f'{v:.1f}%', ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Event analysis complete!\")"
      ],
      "metadata": {
        "id": "GQ5Rpa1Wp2EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean Portfolio Data"
      ],
      "metadata": {
        "id": "9VV84ObpqBbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "STEP 1.1: Clean and Prepare Portfolio (Offer) Data\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ§¹ CLEANING PORTFOLIO DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "portfolio_clean = portfolio.copy()\n",
        "\n",
        "# Rename 'id' to 'offer_id' for clarity\n",
        "portfolio_clean = portfolio_clean.rename(columns={\n",
        "    'id': 'offer_id',\n",
        "    'difficulty': 'min_spend_required',\n",
        "    'duration': 'duration_days'\n",
        "})\n",
        "\n",
        "# Expand the channels column (it's a list) into separate binary columns\n",
        "print(\"\\n Processing channels...\")\n",
        "for channel in ['email', 'mobile', 'social', 'web']:\n",
        "    portfolio_clean[f'channel_{channel}'] = portfolio_clean['channels'].apply(\n",
        "        lambda x: 1 if channel in x else 0\n",
        "    )\n",
        "\n",
        "# Create simplified offer type labels\n",
        "portfolio_clean['offer_type_simple'] = portfolio_clean['offer_type'].map({\n",
        "    'bogo': 'BOGO',\n",
        "    'discount': 'Discount',\n",
        "    'informational': 'Informational'\n",
        "})\n",
        "\n",
        "print(\"\\n Portfolio Data Cleaned!\")\n",
        "print(\"-\" * 80)\n",
        "display(portfolio_clean[['offer_id', 'offer_type_simple', 'reward',\n",
        "                         'min_spend_required', 'duration_days',\n",
        "                         'channel_email', 'channel_mobile', 'channel_social', 'channel_web']].head())\n",
        "\n",
        "print(f\"\\n Offer Type Distribution:\")\n",
        "print(portfolio_clean['offer_type_simple'].value_counts())\n",
        "\n",
        "# Visualize offer types\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Offer type distribution\n",
        "portfolio_clean['offer_type_simple'].value_counts().plot(\n",
        "    kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black'\n",
        ")\n",
        "axes[0].set_title('Offer Type Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Offer Type')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].tick_params(axis='x', rotation=0)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Reward distribution by offer type\n",
        "portfolio_clean.groupby('offer_type_simple')['reward'].mean().plot(\n",
        "    kind='bar', ax=axes[1], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black'\n",
        ")\n",
        "axes[1].set_title('Average Reward by Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Offer Type')\n",
        "axes[1].set_ylabel('Average Reward ($)')\n",
        "axes[1].tick_params(axis='x', rotation=0)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Duration distribution\n",
        "portfolio_clean.groupby('offer_type_simple')['duration_days'].mean().plot(\n",
        "    kind='bar', ax=axes[2], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black'\n",
        ")\n",
        "axes[2].set_title('Average Duration by Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[2].set_xlabel('Offer Type')\n",
        "axes[2].set_ylabel('Duration (Days)')\n",
        "axes[2].tick_params(axis='x', rotation=0)\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mP9AKaAYp7Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean Profile Data"
      ],
      "metadata": {
        "id": "wXoJSHPEqHTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "STEP 1.2: Clean and Prepare Profile (Customer) Data\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ§¹ CLEANING PROFILE DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "profile_clean = profile.copy()\n",
        "\n",
        "# Rename 'id' to 'customer_id' for clarity\n",
        "profile_clean = profile_clean.rename(columns={'id': 'customer_id'})\n",
        "\n",
        "# Convert became_member_on to datetime\n",
        "print(\"\\n Converting membership dates...\")\n",
        "profile_clean['became_member_on'] = pd.to_datetime(\n",
        "    profile_clean['became_member_on'],\n",
        "    format='%Y%m%d'\n",
        ")\n",
        "\n",
        "# Handle outlier ages (ages like 118 are likely data errors)\n",
        "print(\"\\n Handling age outliers...\")\n",
        "print(f\"Age range before cleaning: {profile_clean['age'].min()} - {profile_clean['age'].max()}\")\n",
        "profile_clean['age_clean'] = profile_clean['age'].apply(\n",
        "    lambda x: x if (x >= 18 and x <= 100) else np.nan\n",
        ")\n",
        "print(f\"Age range after cleaning: {profile_clean['age_clean'].min()} - {profile_clean['age_clean'].max()}\")\n",
        "\n",
        "# Create age buckets\n",
        "profile_clean['age_bucket'] = pd.cut(\n",
        "    profile_clean['age_clean'],\n",
        "    bins=[0, 25, 35, 45, 55, 65, 100],\n",
        "    labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
        ")\n",
        "\n",
        "# Handle missing income (impute with median by gender)\n",
        "print(\"\\nðŸ’° Handling income data...\")\n",
        "profile_clean['income_clean'] = profile_clean.groupby('gender')['income'].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "\n",
        "# Create income buckets\n",
        "profile_clean['income_bucket'] = pd.cut(\n",
        "    profile_clean['income_clean'],\n",
        "    bins=[0, 40000, 60000, 80000, 100000, 150000],\n",
        "    labels=['<40K', '40-60K', '60-80K', '80-100K', '100K+']\n",
        ")\n",
        "\n",
        "# Handle missing gender\n",
        "profile_clean['gender'] = profile_clean['gender'].fillna('Unknown')\n",
        "\n",
        "# Calculate member tenure (we'll use a reference date - assume data collection end of July 2018)\n",
        "REFERENCE_DATE = pd.to_datetime('2018-07-31')\n",
        "profile_clean['member_tenure_days'] = (\n",
        "    REFERENCE_DATE - profile_clean['became_member_on']\n",
        ").dt.days\n",
        "\n",
        "# Create tenure buckets\n",
        "profile_clean['tenure_bucket'] = pd.cut(\n",
        "    profile_clean['member_tenure_days'],\n",
        "    bins=[0, 365, 730, 1095, 10000],\n",
        "    labels=['<1yr', '1-2yr', '2-3yr', '3yr+']\n",
        ")\n",
        "\n",
        "print(\"\\n Profile Data Cleaned!\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\nMissing values after cleaning:\")\n",
        "print(profile_clean[['customer_id', 'gender', 'age_clean', 'income_clean',\n",
        "                     'became_member_on', 'member_tenure_days']].isnull().sum())\n",
        "\n",
        "print(f\"\\n Customer Summary:\")\n",
        "print(f\"Total customers: {len(profile_clean):,}\")\n",
        "print(f\"\\nGender distribution:\")\n",
        "print(profile_clean['gender'].value_counts())\n",
        "print(f\"\\nAge distribution:\")\n",
        "print(profile_clean['age_bucket'].value_counts().sort_index())\n",
        "print(f\"\\nIncome distribution:\")\n",
        "print(profile_clean['income_bucket'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "NYganGa3qGKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Customer Demographics"
      ],
      "metadata": {
        "id": "3OeIg3k5qPnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Visualize cleaned customer demographics\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CUSTOMER DEMOGRAPHICS VISUALIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# Gender distribution\n",
        "profile_clean['gender'].value_counts().plot(\n",
        "    kind='bar', ax=axes[0, 0], color=['#3498db', '#e74c3c', '#95a5a6'], edgecolor='black'\n",
        ")\n",
        "axes[0, 0].set_title('Gender Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Gender')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].tick_params(axis='x', rotation=0)\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Age distribution\n",
        "profile_clean['age_bucket'].value_counts().sort_index().plot(\n",
        "    kind='bar', ax=axes[0, 1], color='#9b59b6', edgecolor='black'\n",
        ")\n",
        "axes[0, 1].set_title('Age Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Age Group')\n",
        "axes[0, 1].set_ylabel('Count')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Income distribution\n",
        "profile_clean['income_bucket'].value_counts().sort_index().plot(\n",
        "    kind='bar', ax=axes[0, 2], color='#1abc9c', edgecolor='black'\n",
        ")\n",
        "axes[0, 2].set_title('Income Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].set_xlabel('Income Range')\n",
        "axes[0, 2].set_ylabel('Count')\n",
        "axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "axes[0, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Member tenure distribution\n",
        "profile_clean['tenure_bucket'].value_counts().sort_index().plot(\n",
        "    kind='bar', ax=axes[1, 0], color='#e67e22', edgecolor='black'\n",
        ")\n",
        "axes[1, 0].set_title('Member Tenure Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Tenure')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "axes[1, 0].tick_params(axis='x', rotation=0)\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Age histogram (continuous)\n",
        "profile_clean['age_clean'].dropna().hist(ax=axes[1, 1], bins=30, color='#34495e', edgecolor='black')\n",
        "axes[1, 1].set_title('Age Distribution (Continuous)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Age')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Income histogram (continuous)\n",
        "profile_clean['income_clean'].dropna().hist(ax=axes[1, 2], bins=30, color='#16a085', edgecolor='black')\n",
        "axes[1, 2].set_title('Income Distribution (Continuous)', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_xlabel('Income ($)')\n",
        "axes[1, 2].set_ylabel('Frequency')\n",
        "axes[1, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Demographics visualization complete!\")"
      ],
      "metadata": {
        "id": "mc8JHBvJqK7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean Transcript Data"
      ],
      "metadata": {
        "id": "CkkTmz6kqaBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "STEP 1.3: Clean and Prepare Transcript (Event Log) Data\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CLEANING TRANSCRIPT DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "transcript_clean = transcript.copy()\n",
        "\n",
        "# Rename 'person' to 'customer_id' for consistency\n",
        "transcript_clean = transcript_clean.rename(columns={'person': 'customer_id'})\n",
        "\n",
        "# Expand the 'value' column (it's a dictionary)\n",
        "print(\"\\n Expanding 'value' column...\")\n",
        "value_expanded = transcript_clean['value'].apply(pd.Series)\n",
        "transcript_clean = pd.concat([transcript_clean.drop('value', axis=1), value_expanded], axis=1)\n",
        "\n",
        "print(f\"New columns from 'value': {value_expanded.columns.tolist()}\")\n",
        "\n",
        "# Convert time (hours) to actual timestamp\n",
        "print(\"\\n Converting time to timestamps...\")\n",
        "# Time is in hours since start of experiment (assume start = 2018-01-01)\n",
        "START_DATE = pd.to_datetime('2018-01-01')\n",
        "transcript_clean['timestamp'] = START_DATE + pd.to_timedelta(transcript_clean['time'], unit='h')\n",
        "transcript_clean['days_since_start'] = transcript_clean['time'] / 24\n",
        "\n",
        "print(\"\\n Transcript Data Cleaned!\")\n",
        "print(\"-\" * 80)\n",
        "display(transcript_clean.head(10))\n",
        "\n",
        "print(f\"\\n Event Summary:\")\n",
        "print(f\"Total events: {len(transcript_clean):,}\")\n",
        "print(f\"Unique customers: {transcript_clean['customer_id'].nunique():,}\")\n",
        "print(f\"Date range: {transcript_clean['timestamp'].min()} to {transcript_clean['timestamp'].max()}\")\n",
        "print(f\"Duration: {(transcript_clean['timestamp'].max() - transcript_clean['timestamp'].min()).days} days\")\n",
        "\n",
        "print(f\"\\n Event Type Breakdown:\")\n",
        "print(transcript_clean['event'].value_counts())"
      ],
      "metadata": {
        "id": "7ibmzaWwqS0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze Event Patterns"
      ],
      "metadata": {
        "id": "J7rpQ7Wjqfoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze temporal patterns in the event log\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TEMPORAL EVENT ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Events over time\n",
        "events_by_day = transcript_clean.groupby(\n",
        "    transcript_clean['timestamp'].dt.date\n",
        ").size().reset_index(name='event_count')\n",
        "events_by_day['timestamp'] = pd.to_datetime(events_by_day['timestamp'])\n",
        "\n",
        "# Plot events over time\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
        "\n",
        "# Total events over time\n",
        "axes[0].plot(events_by_day['timestamp'], events_by_day['event_count'],\n",
        "             color='#3498db', linewidth=2)\n",
        "axes[0].set_title('Total Events Over Time', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Date')\n",
        "axes[0].set_ylabel('Number of Events')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].fill_between(events_by_day['timestamp'], events_by_day['event_count'],\n",
        "                      alpha=0.3, color='#3498db')\n",
        "\n",
        "# Events by type over time\n",
        "events_by_type_time = transcript_clean.groupby(\n",
        "    [transcript_clean['timestamp'].dt.date, 'event']\n",
        ").size().reset_index(name='count')\n",
        "events_by_type_time['timestamp'] = pd.to_datetime(events_by_type_time['timestamp'])\n",
        "\n",
        "for event_type in transcript_clean['event'].unique():\n",
        "    data = events_by_type_time[events_by_type_time['event'] == event_type]\n",
        "    axes[1].plot(data['timestamp'], data['count'], label=event_type, linewidth=2, marker='o', markersize=3)\n",
        "\n",
        "axes[1].set_title('Events by Type Over Time', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Date')\n",
        "axes[1].set_ylabel('Number of Events')\n",
        "axes[1].legend(loc='upper left')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Temporal analysis complete!\")"
      ],
      "metadata": {
        "id": "t__WVlrkqdiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Decision Framework"
      ],
      "metadata": {
        "id": "AVIKE97oq8m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "STEP 1.4: Define Decision Framework and Business Rules\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" DECISION FRAMEWORK DEFINITION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        "DECISION FRAMEWORK FOR NEXT BEST ACTION\n",
        "========================================\n",
        "\n",
        " DECISION MOMENT:\n",
        "   When an offer is received by a customer, that timestamp becomes \"today\"\n",
        "   We use only data BEFORE this moment to make predictions\n",
        "\n",
        " ACTIONS AVAILABLE:\n",
        "   1. BOGO (Buy One Get One)\n",
        "   2. Discount (Percentage off)\n",
        "   3. Informational (Awareness only, no reward)\n",
        "   4. No Offer (Control group)\n",
        "\n",
        " OUTCOMES TO MEASURE:\n",
        "\n",
        "   PRIMARY OUTCOMES (within evaluation windows):\n",
        "\n",
        "   1. CONVERSION (Short-term - 14 days):\n",
        "      - Did customer complete the offer OR make any purchase?\n",
        "      - Binary: Yes (1) / No (0)\n",
        "\n",
        "   2. RETENTION (Long-term - 30 days):\n",
        "      - Number of transactions in next 30 days\n",
        "      - Binary: Active (1) / Inactive (0)\n",
        "\n",
        " BUSINESS GOAL:\n",
        "   Maximize incremental profit = Revenue uplift - Offer cost\n",
        "\"\"\")\n",
        "\n",
        "# Define evaluation windows\n",
        "CONVERSION_WINDOW_DAYS = 14\n",
        "RETENTION_WINDOW_DAYS = 30\n",
        "\n",
        "print(f\"\\n  EVALUATION WINDOWS:\")\n",
        "print(f\"   â€¢ Conversion window: {CONVERSION_WINDOW_DAYS} days\")\n",
        "print(f\"   â€¢ Retention window: {RETENTION_WINDOW_DAYS} days\")\n",
        "\n",
        "# Define offer costs (assumptions for profit calculation)\n",
        "OFFER_COSTS = {\n",
        "    'BOGO': 5.0,           # Cost of giving away free item\n",
        "    'Discount': 3.0,       # Average discount amount\n",
        "    'Informational': 0.5,  # Minimal cost for sending notification\n",
        "    'No Offer': 0.0        # No cost\n",
        "}\n",
        "\n",
        "print(f\"\\n ASSUMED OFFER COSTS:\")\n",
        "for offer_type, cost in OFFER_COSTS.items():\n",
        "    print(f\"   â€¢ {offer_type:20s}: ${cost:.2f}\")\n",
        "\n",
        "# Store these as constants for later use\n",
        "print(\"\\n Decision framework defined!\")\n",
        "print(\"\\n These parameters will be used throughout the modeling process\")"
      ],
      "metadata": {
        "id": "ecNLbpmdqv-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Offer-Customer Interaction Mapping"
      ],
      "metadata": {
        "id": "C_ediecZrDb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "STEP 1.5: Create Offer-Customer Interaction Mapping\n",
        "This creates our \"decision moments\" - every time an offer was received\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CREATING OFFER-CUSTOMER INTERACTION MAPPING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Filter for 'offer received' events - these are our decision moments\n",
        "offers_received = transcript_clean[\n",
        "    transcript_clean['event'] == 'offer received'\n",
        "].copy()\n",
        "\n",
        "print(f\"\\n Total 'offer received' events: {len(offers_received):,}\")\n",
        "print(f\"   Unique customers who received offers: {offers_received['customer_id'].nunique():,}\")\n",
        "print(f\"   Date range: {offers_received['timestamp'].min()} to {offers_received['timestamp'].max()}\")\n",
        "\n",
        "# Check what columns we actually have after expanding 'value'\n",
        "print(f\"\\n Available columns in offers_received:\")\n",
        "print(offers_received.columns.tolist())\n",
        "\n",
        "# The column might be 'offer id' (with space) - let's check and handle it\n",
        "if 'offer id' in offers_received.columns:\n",
        "    # Just rename it, don't drop the existing offer_id if it exists\n",
        "    offers_received['offer_id_from_value'] = offers_received['offer id']\n",
        "    print(\" Created 'offer_id_from_value' from 'offer id' column\")\n",
        "\n",
        "    # Use this column for merging\n",
        "    offer_id_col = 'offer_id_from_value'\n",
        "elif 'offer_id' in offers_received.columns:\n",
        "    offer_id_col = 'offer_id'\n",
        "    print(\" Using existing 'offer_id' column\")\n",
        "else:\n",
        "    print(\" ERROR: Cannot find offer id column!\")\n",
        "    print(f\"Available columns: {offers_received.columns.tolist()}\")\n",
        "\n",
        "# Drop the old offer_id column if it exists and is causing issues\n",
        "if 'offer_id' in offers_received.columns and offer_id_col == 'offer_id_from_value':\n",
        "    offers_received = offers_received.drop(columns=['offer_id'])\n",
        "    print(\" Dropped duplicate 'offer_id' column\")\n",
        "\n",
        "# Rename our working column to offer_id\n",
        "if offer_id_col == 'offer_id_from_value':\n",
        "    offers_received = offers_received.rename(columns={'offer_id_from_value': 'offer_id'})\n",
        "\n",
        "# Now merge with portfolio to get offer details\n",
        "offers_received = offers_received.merge(\n",
        "    portfolio_clean[['offer_id', 'offer_type_simple', 'reward', 'min_spend_required', 'duration_days']],\n",
        "    on='offer_id',\n",
        "    how='left',\n",
        "    suffixes=('', '_portfolio')  # Add suffix to avoid column name conflicts\n",
        ")\n",
        "\n",
        "print(f\"\\n Merged with portfolio data\")\n",
        "print(f\"\\n Offers received by type:\")\n",
        "print(offers_received['offer_type_simple'].value_counts())\n",
        "\n",
        "# Create a unique identifier for each offer instance\n",
        "# (same customer can receive same offer multiple times)\n",
        "offers_received['offer_instance_id'] = (\n",
        "    offers_received['customer_id'].astype(str) + '_' +\n",
        "    offers_received['offer_id'].astype(str) + '_' +\n",
        "    offers_received['timestamp'].astype(str)\n",
        ")\n",
        "\n",
        "print(f\"\\n Created {len(offers_received):,} unique offer instances\")\n",
        "\n",
        "# Preview the offer instances\n",
        "print(\"\\n Sample of offer instances:\")\n",
        "display(offers_received[['customer_id', 'timestamp', 'offer_id', 'offer_type_simple',\n",
        "                         'reward', 'min_spend_required', 'duration_days']].head(10))\n",
        "\n",
        "print(f\"\\n Final columns in offers_received:\")\n",
        "print(offers_received.columns.tolist())"
      ],
      "metadata": {
        "id": "ShiPBB92q-TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze Offer Received Patterns"
      ],
      "metadata": {
        "id": "oWF-AcIespX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze patterns in offer distribution\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" OFFER DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# How many offers does each customer receive on average?\n",
        "offers_per_customer = offers_received.groupby('customer_id').size()\n",
        "\n",
        "print(f\"\\n Offers per customer statistics:\")\n",
        "print(f\"   Mean: {offers_per_customer.mean():.2f}\")\n",
        "print(f\"   Median: {offers_per_customer.median():.0f}\")\n",
        "print(f\"   Min: {offers_per_customer.min()}\")\n",
        "print(f\"   Max: {offers_per_customer.max()}\")\n",
        "print(f\"   Std: {offers_per_customer.std():.2f}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Distribution of offers per customer\n",
        "axes[0].hist(offers_per_customer, bins=30, color='#3498db', edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('Distribution of Offers per Customer', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Number of Offers Received')\n",
        "axes[0].set_ylabel('Number of Customers')\n",
        "axes[0].axvline(offers_per_customer.mean(), color='red', linestyle='--',\n",
        "                linewidth=2, label=f'Mean: {offers_per_customer.mean():.1f}')\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Offers by type over time\n",
        "offers_by_type_date = offers_received.groupby(\n",
        "    [offers_received['timestamp'].dt.date, 'offer_type_simple']\n",
        ").size().reset_index(name='count')\n",
        "offers_by_type_date['date'] = pd.to_datetime(offers_by_type_date['timestamp'])\n",
        "\n",
        "for offer_type in offers_received['offer_type_simple'].dropna().unique():\n",
        "    data = offers_by_type_date[offers_by_type_date['offer_type_simple'] == offer_type]\n",
        "    axes[1].plot(data['date'], data['count'], label=offer_type, linewidth=2, marker='o', markersize=4)\n",
        "\n",
        "axes[1].set_title('Offers Sent by Type Over Time', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Date')\n",
        "axes[1].set_ylabel('Number of Offers Sent')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Offer type distribution (pie chart)\n",
        "offer_type_counts = offers_received['offer_type_simple'].value_counts()\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "axes[2].pie(offer_type_counts.values, labels=offer_type_counts.index, autopct='%1.1f%%',\n",
        "            colors=colors, startangle=90, wedgeprops={'edgecolor': 'black', 'linewidth': 1.5})\n",
        "axes[2].set_title('Offer Type Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Offer distribution analysis complete!\")"
      ],
      "metadata": {
        "id": "w4wTBXp_rGpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Offer Viewed and Completed Events"
      ],
      "metadata": {
        "id": "wonFQuUnswYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Extract offer viewed and offer completed events\n",
        "These will be used to calculate conversion outcomes\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" EXTRACTING OFFER VIEWED AND COMPLETED EVENTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Offer viewed events\n",
        "offers_viewed = transcript_clean[\n",
        "    transcript_clean['event'] == 'offer viewed'\n",
        "].copy()\n",
        "\n",
        "# Rename column if needed\n",
        "if 'offer id' in offers_viewed.columns:\n",
        "    offers_viewed = offers_viewed.rename(columns={'offer id': 'offer_id'})\n",
        "\n",
        "print(f\"\\n  Offer viewed events: {len(offers_viewed):,}\")\n",
        "print(f\"   Unique customers: {offers_viewed['customer_id'].nunique():,}\")\n",
        "\n",
        "# Offer completed events\n",
        "offers_completed = transcript_clean[\n",
        "    transcript_clean['event'] == 'offer completed'\n",
        "].copy()\n",
        "\n",
        "# Rename column if needed\n",
        "if 'offer id' in offers_completed.columns:\n",
        "    offers_completed = offers_completed.rename(columns={'offer id': 'offer_id'})\n",
        "\n",
        "print(f\"\\n Offer completed events: {len(offers_completed):,}\")\n",
        "print(f\"   Unique customers: {offers_completed['customer_id'].nunique():,}\")\n",
        "\n",
        "# View rate: % of received offers that were viewed\n",
        "total_received = len(offers_received)\n",
        "total_viewed = len(offers_viewed)\n",
        "view_rate = (total_viewed / total_received) * 100\n",
        "\n",
        "print(f\"\\n Overall view rate: {view_rate:.2f}%\")\n",
        "\n",
        "# Completion rate: % of received offers that were completed\n",
        "total_completed = len(offers_completed)\n",
        "completion_rate = (total_completed / total_received) * 100\n",
        "\n",
        "print(f\" Overall completion rate: {completion_rate:.2f}%\")\n",
        "\n",
        "# Completion rate given view\n",
        "if total_viewed > 0:\n",
        "    completion_given_view = (total_completed / total_viewed) * 100\n",
        "    print(f\" Completion rate (given view): {completion_given_view:.2f}%\")\n",
        "\n",
        "# Visualize the funnel\n",
        "funnel_data = pd.DataFrame({\n",
        "    'Stage': ['Received', 'Viewed', 'Completed'],\n",
        "    'Count': [total_received, total_viewed, total_completed]\n",
        "})\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "colors_funnel = ['#3498db', '#2ecc71', '#e74c3c']\n",
        "bars = ax.barh(funnel_data['Stage'], funnel_data['Count'], color=colors_funnel, edgecolor='black')\n",
        "\n",
        "# Add percentage labels\n",
        "for i, (stage, count) in enumerate(zip(funnel_data['Stage'], funnel_data['Count'])):\n",
        "    percentage = (count / total_received) * 100\n",
        "    ax.text(count + 1000, i, f'{count:,} ({percentage:.1f}%)',\n",
        "            va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_xlabel('Number of Events', fontsize=12)\n",
        "ax.set_title('Offer Engagement Funnel', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_fxy7epKsqZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Extract transaction (purchase) events\n",
        "These will be used for calculating retention and spending outcomes\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" EXTRACTING TRANSACTION EVENTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Transaction events\n",
        "transactions = transcript_clean[\n",
        "    transcript_clean['event'] == 'transaction'\n",
        "].copy()\n",
        "\n",
        "print(f\"\\n Total transactions: {len(transactions):,}\")\n",
        "print(f\"   Unique customers who transacted: {transactions['customer_id'].nunique():,}\")\n",
        "print(f\"   Date range: {transactions['timestamp'].min()} to {transactions['timestamp'].max()}\")\n",
        "\n",
        "# Transaction statistics\n",
        "print(f\"\\n Transaction Amount Statistics:\")\n",
        "print(f\"   Mean: ${transactions['amount'].mean():.2f}\")\n",
        "print(f\"   Median: ${transactions['amount'].median():.2f}\")\n",
        "print(f\"   Min: ${transactions['amount'].min():.2f}\")\n",
        "print(f\"   Max: ${transactions['amount'].max():.2f}\")\n",
        "print(f\"   Total revenue: ${transactions['amount'].sum():,.2f}\")\n",
        "\n",
        "# Transactions per customer\n",
        "txns_per_customer = transactions.groupby('customer_id').size()\n",
        "print(f\"\\n Transactions per customer:\")\n",
        "print(f\"   Mean: {txns_per_customer.mean():.2f}\")\n",
        "print(f\"   Median: {txns_per_customer.median():.0f}\")\n",
        "print(f\"   Min: {txns_per_customer.min()}\")\n",
        "print(f\"   Max: {txns_per_customer.max()}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Transaction amount distribution\n",
        "axes[0].hist(transactions['amount'], bins=50, color='#2ecc71', edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('Transaction Amount Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Transaction Amount ($)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].axvline(transactions['amount'].mean(), color='red', linestyle='--',\n",
        "                linewidth=2, label=f'Mean: ${transactions[\"amount\"].mean():.2f}')\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Transactions per customer distribution\n",
        "axes[1].hist(txns_per_customer, bins=30, color='#9b59b6', edgecolor='black', alpha=0.7)\n",
        "axes[1].set_title('Transactions per Customer Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Number of Transactions')\n",
        "axes[1].set_ylabel('Number of Customers')\n",
        "axes[1].axvline(txns_per_customer.mean(), color='red', linestyle='--',\n",
        "                linewidth=2, label=f'Mean: {txns_per_customer.mean():.1f}')\n",
        "axes[1].legend()\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Transactions over time\n",
        "txns_by_date = transactions.groupby(transactions['timestamp'].dt.date).agg({\n",
        "    'amount': ['count', 'sum']\n",
        "}).reset_index()\n",
        "txns_by_date.columns = ['date', 'count', 'revenue']\n",
        "txns_by_date['date'] = pd.to_datetime(txns_by_date['date'])\n",
        "\n",
        "ax2 = axes[2]\n",
        "ax2.plot(txns_by_date['date'], txns_by_date['count'],\n",
        "         color='#e74c3c', linewidth=2, label='Transaction Count')\n",
        "ax2.set_xlabel('Date')\n",
        "ax2.set_ylabel('Transaction Count', color='#e74c3c')\n",
        "ax2.tick_params(axis='y', labelcolor='#e74c3c')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "ax2_twin = ax2.twinx()\n",
        "ax2_twin.plot(txns_by_date['date'], txns_by_date['revenue'],\n",
        "              color='#3498db', linewidth=2, linestyle='--', label='Revenue')\n",
        "ax2_twin.set_ylabel('Revenue ($)', color='#3498db')\n",
        "ax2_twin.tick_params(axis='y', labelcolor='#3498db')\n",
        "\n",
        "axes[2].set_title('Transactions and Revenue Over Time', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Combine legends\n",
        "lines1, labels1 = ax2.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
        "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Transaction analysis complete!\")"
      ],
      "metadata": {
        "id": "C11LgumesyvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customer Activity Summary"
      ],
      "metadata": {
        "id": "kHHWZMuys8H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create a comprehensive customer activity summary\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CUSTOMER ACTIVITY SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Aggregate all customer activities\n",
        "customer_summary = pd.DataFrame({\n",
        "    'customer_id': profile_clean['customer_id']\n",
        "})\n",
        "\n",
        "# Merge with profile data\n",
        "customer_summary = customer_summary.merge(\n",
        "    profile_clean[['customer_id', 'gender', 'age_clean', 'income_clean',\n",
        "                   'became_member_on', 'member_tenure_days', 'age_bucket',\n",
        "                   'income_bucket', 'tenure_bucket']],\n",
        "    on='customer_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Add offer statistics\n",
        "offer_stats = offers_received.groupby('customer_id').agg({\n",
        "    'offer_instance_id': 'count',\n",
        "    'offer_type_simple': lambda x: x.mode()[0] if len(x.mode()) > 0 else None\n",
        "}).rename(columns={\n",
        "    'offer_instance_id': 'total_offers_received',\n",
        "    'offer_type_simple': 'most_common_offer_type'\n",
        "}).reset_index()\n",
        "customer_summary = customer_summary.merge(offer_stats, on='customer_id', how='left')\n",
        "\n",
        "# Add view statistics\n",
        "view_stats = offers_viewed.groupby('customer_id').size().reset_index(name='total_offers_viewed')\n",
        "customer_summary = customer_summary.merge(view_stats, on='customer_id', how='left')\n",
        "\n",
        "# Add completion statistics\n",
        "completion_stats = offers_completed.groupby('customer_id').size().reset_index(name='total_offers_completed')\n",
        "customer_summary = customer_summary.merge(completion_stats, on='customer_id', how='left')\n",
        "\n",
        "# Add transaction statistics\n",
        "txn_stats = transactions.groupby('customer_id').agg({\n",
        "    'amount': ['count', 'sum', 'mean', 'max']\n",
        "}).reset_index()\n",
        "txn_stats.columns = ['customer_id', 'total_transactions', 'total_spent', 'avg_transaction', 'max_transaction']\n",
        "customer_summary = customer_summary.merge(txn_stats, on='customer_id', how='left')\n",
        "\n",
        "# Fill NaN values with 0 for counts\n",
        "count_columns = ['total_offers_received', 'total_offers_viewed', 'total_offers_completed',\n",
        "                 'total_transactions', 'total_spent', 'avg_transaction', 'max_transaction']\n",
        "customer_summary[count_columns] = customer_summary[count_columns].fillna(0)\n",
        "\n",
        "# Calculate engagement rates\n",
        "customer_summary['offer_view_rate'] = np.where(\n",
        "    customer_summary['total_offers_received'] > 0,\n",
        "    customer_summary['total_offers_viewed'] / customer_summary['total_offers_received'],\n",
        "    0\n",
        ")\n",
        "\n",
        "customer_summary['offer_completion_rate'] = np.where(\n",
        "    customer_summary['total_offers_received'] > 0,\n",
        "    customer_summary['total_offers_completed'] / customer_summary['total_offers_received'],\n",
        "    0\n",
        ")\n",
        "\n",
        "print(f\"\\n Customer summary created for {len(customer_summary):,} customers\")\n",
        "print(f\"\\nSummary statistics:\")\n",
        "print(customer_summary[['total_offers_received', 'total_offers_viewed',\n",
        "                        'total_offers_completed', 'total_transactions',\n",
        "                        'total_spent']].describe())\n",
        "\n",
        "# Preview\n",
        "print(\"\\n Sample customer summaries:\")\n",
        "display(customer_summary.head(10))"
      ],
      "metadata": {
        "id": "X1XH-St6s1z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Cleaned Data"
      ],
      "metadata": {
        "id": "dlUEtJO4s_26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Save all cleaned datasets for next steps\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SAVING CLEANED DATASETS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create a data folder if it doesn't exist\n",
        "os.makedirs('cleaned_data', exist_ok=True)\n",
        "\n",
        "# Save all cleaned datasets\n",
        "portfolio_clean.to_csv('cleaned_data/portfolio_clean.csv', index=False)\n",
        "print(\" Saved: portfolio_clean.csv\")\n",
        "\n",
        "profile_clean.to_csv('cleaned_data/profile_clean.csv', index=False)\n",
        "print(\" Saved: profile_clean.csv\")\n",
        "\n",
        "transcript_clean.to_csv('cleaned_data/transcript_clean.csv', index=False)\n",
        "print(\" Saved: transcript_clean.csv\")\n",
        "\n",
        "offers_received.to_csv('cleaned_data/offers_received.csv', index=False)\n",
        "print(\" Saved: offers_received.csv\")\n",
        "\n",
        "offers_viewed.to_csv('cleaned_data/offers_viewed.csv', index=False)\n",
        "print(\" Saved: offers_viewed.csv\")\n",
        "\n",
        "offers_completed.to_csv('cleaned_data/offers_completed.csv', index=False)\n",
        "print(\" Saved: offers_completed.csv\")\n",
        "\n",
        "transactions.to_csv('cleaned_data/transactions.csv', index=False)\n",
        "print(\" Saved: transactions.csv\")\n",
        "\n",
        "customer_summary.to_csv('cleaned_data/customer_summary.csv', index=False)\n",
        "print(\" Saved: customer_summary.csv\")\n",
        "\n",
        "print(f\"\\n All datasets saved to 'cleaned_data/' folder\")\n",
        "\n",
        "# List saved files\n",
        "print(\"\\n Saved files:\")\n",
        "!ls -lh cleaned_data/"
      ],
      "metadata": {
        "id": "J7dak2Rfs8x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Base modelling dataset"
      ],
      "metadata": {
        "id": "MGXnMnRitRQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create the base modeling dataset - one row per offer instance\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CREATING BASE MODELING DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Start with offers_received as our base\n",
        "# Each row = one decision moment (customer received an offer)\n",
        "modeling_data = offers_received.copy()\n",
        "\n",
        "print(f\"\\n Base dataset created:\")\n",
        "print(f\"   Total rows: {len(modeling_data):,}\")\n",
        "print(f\"   Columns: {modeling_data.shape[1]}\")\n",
        "\n",
        "# Add customer demographic features\n",
        "print(\"\\n Merging customer demographics...\")\n",
        "modeling_data = modeling_data.merge(\n",
        "    profile_clean[['customer_id', 'gender', 'age_clean', 'income_clean',\n",
        "                   'member_tenure_days', 'became_member_on']],\n",
        "    on='customer_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Calculate tenure at the time of offer (not current tenure)\n",
        "modeling_data['tenure_at_offer'] = (\n",
        "    modeling_data['timestamp'] - modeling_data['became_member_on']\n",
        ").dt.days\n",
        "\n",
        "print(f\" Demographics added\")\n",
        "print(f\"   Current columns: {modeling_data.shape[1]}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\n Sample of base modeling data:\")\n",
        "display(modeling_data[['customer_id', 'timestamp', 'offer_type_simple',\n",
        "                       'gender', 'age_clean', 'income_clean', 'tenure_at_offer']].head())"
      ],
      "metadata": {
        "id": "P4lqzORLtCky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Function for Time-Based Features"
      ],
      "metadata": {
        "id": "K_TbvQbNtdm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create helper functions for time-based feature calculation\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"  CREATING HELPER FUNCTIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def calculate_rfm_features(customer_id, decision_time, transactions_df, lookback_days=None):\n",
        "    \"\"\"\n",
        "    Calculate RFM (Recency, Frequency, Monetary) features for a customer\n",
        "    using only transactions BEFORE the decision time.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    customer_id : str\n",
        "        Customer identifier\n",
        "    decision_time : datetime\n",
        "        The decision moment (offer received time)\n",
        "    transactions_df : DataFrame\n",
        "        All transactions data\n",
        "    lookback_days : int or None\n",
        "        If specified, only look back this many days. If None, use all history.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary of RFM features\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter transactions for this customer BEFORE decision time\n",
        "    customer_txns = transactions_df[\n",
        "        (transactions_df['customer_id'] == customer_id) &\n",
        "        (transactions_df['timestamp'] < decision_time)\n",
        "    ].copy()\n",
        "\n",
        "    # If lookback_days specified, further filter\n",
        "    if lookback_days is not None:\n",
        "        cutoff_time = decision_time - pd.Timedelta(days=lookback_days)\n",
        "        customer_txns = customer_txns[customer_txns['timestamp'] >= cutoff_time]\n",
        "\n",
        "    # Calculate features\n",
        "    if len(customer_txns) == 0:\n",
        "        return {\n",
        "            'recency_days': np.nan,\n",
        "            'frequency': 0,\n",
        "            'monetary_total': 0,\n",
        "            'monetary_mean': 0,\n",
        "            'monetary_max': 0,\n",
        "            'monetary_std': 0,\n",
        "        }\n",
        "\n",
        "    # Recency: days since last transaction\n",
        "    last_txn_time = customer_txns['timestamp'].max()\n",
        "    recency_days = (decision_time - last_txn_time).total_seconds() / 86400  # Convert to days\n",
        "\n",
        "    # Frequency: number of transactions\n",
        "    frequency = len(customer_txns)\n",
        "\n",
        "    # Monetary: spending statistics\n",
        "    monetary_total = customer_txns['amount'].sum()\n",
        "    monetary_mean = customer_txns['amount'].mean()\n",
        "    monetary_max = customer_txns['amount'].max()\n",
        "    monetary_std = customer_txns['amount'].std() if len(customer_txns) > 1 else 0\n",
        "\n",
        "    return {\n",
        "        'recency_days': recency_days,\n",
        "        'frequency': frequency,\n",
        "        'monetary_total': monetary_total,\n",
        "        'monetary_mean': monetary_mean,\n",
        "        'monetary_max': monetary_max,\n",
        "        'monetary_std': monetary_std,\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_offer_history_features(customer_id, decision_time,\n",
        "                                     offers_received_df, offers_viewed_df,\n",
        "                                     offers_completed_df):\n",
        "    \"\"\"\n",
        "    Calculate offer interaction history features for a customer\n",
        "    using only data BEFORE the decision time.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    customer_id : str\n",
        "        Customer identifier\n",
        "    decision_time : datetime\n",
        "        The decision moment (offer received time)\n",
        "    offers_received_df : DataFrame\n",
        "        All offers received data\n",
        "    offers_viewed_df : DataFrame\n",
        "        All offers viewed data\n",
        "    offers_completed_df : DataFrame\n",
        "        All offers completed data\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary of offer history features\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter offers BEFORE decision time\n",
        "    prev_received = offers_received_df[\n",
        "        (offers_received_df['customer_id'] == customer_id) &\n",
        "        (offers_received_df['timestamp'] < decision_time)\n",
        "    ]\n",
        "\n",
        "    prev_viewed = offers_viewed_df[\n",
        "        (offers_viewed_df['customer_id'] == customer_id) &\n",
        "        (offers_viewed_df['timestamp'] < decision_time)\n",
        "    ]\n",
        "\n",
        "    prev_completed = offers_completed_df[\n",
        "        (offers_completed_df['customer_id'] == customer_id) &\n",
        "        (offers_completed_df['timestamp'] < decision_time)\n",
        "    ]\n",
        "\n",
        "    # Calculate counts\n",
        "    num_prev_received = len(prev_received)\n",
        "    num_prev_viewed = len(prev_viewed)\n",
        "    num_prev_completed = len(prev_completed)\n",
        "\n",
        "    # Calculate rates\n",
        "    view_rate = num_prev_viewed / num_prev_received if num_prev_received > 0 else 0\n",
        "    completion_rate = num_prev_completed / num_prev_received if num_prev_received > 0 else 0\n",
        "    completion_given_view_rate = num_prev_completed / num_prev_viewed if num_prev_viewed > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'prev_offers_received': num_prev_received,\n",
        "        'prev_offers_viewed': num_prev_viewed,\n",
        "        'prev_offers_completed': num_prev_completed,\n",
        "        'prev_view_rate': view_rate,\n",
        "        'prev_completion_rate': completion_rate,\n",
        "        'prev_completion_given_view_rate': completion_given_view_rate,\n",
        "    }\n",
        "\n",
        "print(\" Helper functions created:\")\n",
        "print(\"   â€¢ calculate_rfm_features()\")\n",
        "print(\"   â€¢ calculate_offer_history_features()\")"
      ],
      "metadata": {
        "id": "IksriKtWtQcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate RFM Features"
      ],
      "metadata": {
        "id": "qlrAxK6btkxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate RFM features for each offer instance\n",
        "This is computationally intensive - we'll use vectorized operations where possible\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CALCULATING RFM FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "print(\"\\n This may take a few minutes for 76K+ offer instances...\")\n",
        "print(\"   Processing RFM features for different time windows...\\n\")\n",
        "\n",
        "# We'll calculate RFM for different lookback windows\n",
        "# This gives us both recent and long-term behavior\n",
        "\n",
        "# Initialize feature columns\n",
        "rfm_windows = {\n",
        "    'lifetime': None,  # All history\n",
        "    '90d': 90,         # Last 90 days\n",
        "    '30d': 30,         # Last 30 days\n",
        "    '7d': 7,           # Last 7 days\n",
        "}\n",
        "\n",
        "for window_name, lookback_days in rfm_windows.items():\n",
        "    print(f\" Calculating RFM features for {window_name} window...\")\n",
        "\n",
        "    rfm_features = modeling_data.progress_apply(\n",
        "        lambda row: calculate_rfm_features(\n",
        "            row['customer_id'],\n",
        "            row['timestamp'],\n",
        "            transactions,\n",
        "            lookback_days\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Convert to DataFrame and add to modeling_data\n",
        "    rfm_df = pd.DataFrame(rfm_features.tolist())\n",
        "\n",
        "    # Rename columns with window prefix\n",
        "    rfm_df.columns = [f'rfm_{window_name}_{col}' for col in rfm_df.columns]\n",
        "\n",
        "    # Add to modeling_data\n",
        "    modeling_data = pd.concat([modeling_data, rfm_df], axis=1)\n",
        "\n",
        "    print(f\"    {window_name} features added\")\n",
        "\n",
        "print(f\"\\n All RFM features calculated!\")\n",
        "print(f\"   Total columns now: {modeling_data.shape[1]}\")\n",
        "\n",
        "# Show sample of RFM features\n",
        "print(\"\\n Sample of RFM features:\")\n",
        "rfm_cols = [col for col in modeling_data.columns if 'rfm_' in col]\n",
        "display(modeling_data[['customer_id', 'timestamp'] + rfm_cols[:8]].head())"
      ],
      "metadata": {
        "id": "WVskziz3tYFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Offer History Features"
      ],
      "metadata": {
        "id": "ObaAoA-ut_8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate offer interaction history features\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CALCULATING OFFER HISTORY FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n Processing offer interaction history...\\n\")\n",
        "\n",
        "offer_history_features = modeling_data.progress_apply(\n",
        "    lambda row: calculate_offer_history_features(\n",
        "        row['customer_id'],\n",
        "        row['timestamp'],\n",
        "        offers_received,\n",
        "        offers_viewed,\n",
        "        offers_completed\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Convert to DataFrame\n",
        "offer_history_df = pd.DataFrame(offer_history_features.tolist())\n",
        "\n",
        "# Add to modeling_data\n",
        "modeling_data = pd.concat([modeling_data, offer_history_df], axis=1)\n",
        "\n",
        "print(f\"\\n Offer history features calculated!\")\n",
        "print(f\"   Total columns now: {modeling_data.shape[1]}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n Sample of offer history features:\")\n",
        "offer_hist_cols = [col for col in modeling_data.columns if 'prev_' in col]\n",
        "display(modeling_data[['customer_id', 'timestamp'] + offer_hist_cols].head(10))\n",
        "\n",
        "# Show statistics\n",
        "print(\"\\n Offer history feature statistics:\")\n",
        "print(modeling_data[offer_hist_cols].describe())"
      ],
      "metadata": {
        "id": "CAFQxTDQtldi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Time-Based Features"
      ],
      "metadata": {
        "id": "DV4KiOUiuEVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create time-based features from the offer timestamp\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CREATING TIME-BASED FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract temporal features from timestamp\n",
        "modeling_data['hour_of_day'] = modeling_data['timestamp'].dt.hour\n",
        "modeling_data['day_of_week'] = modeling_data['timestamp'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "modeling_data['day_of_month'] = modeling_data['timestamp'].dt.day\n",
        "modeling_data['month'] = modeling_data['timestamp'].dt.month\n",
        "modeling_data['is_weekend'] = modeling_data['day_of_week'].isin([5, 6]).astype(int)\n",
        "modeling_data['is_month_start'] = (modeling_data['day_of_month'] <= 7).astype(int)\n",
        "modeling_data['is_month_end'] = (modeling_data['day_of_month'] >= 24).astype(int)\n",
        "\n",
        "# Days since experiment start\n",
        "START_DATE = pd.to_datetime('2018-01-01')\n",
        "modeling_data['days_since_experiment_start'] = (\n",
        "    modeling_data['timestamp'] - START_DATE\n",
        ").dt.days\n",
        "\n",
        "print(\" Time-based features created:\")\n",
        "time_features = ['hour_of_day', 'day_of_week', 'day_of_month', 'month',\n",
        "                 'is_weekend', 'is_month_start', 'is_month_end',\n",
        "                 'days_since_experiment_start']\n",
        "for feat in time_features:\n",
        "    print(f\"   â€¢ {feat}\")\n",
        "\n",
        "print(f\"\\n   Total columns now: {modeling_data.shape[1]}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n Sample of time-based features:\")\n",
        "display(modeling_data[['customer_id', 'timestamp'] + time_features].head(10))"
      ],
      "metadata": {
        "id": "eakHC3PcuEtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CREATE A CLEAN COPY OF MODELING_DATA WITH NO INDEX ISSUES\n",
        "This is our checkpoint - we'll work with this clean copy going forward\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CREATING CLEAN COPY OF MODELING_DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n Original modeling_data:\")\n",
        "print(f\"   Shape: {modeling_data.shape}\")\n",
        "print(f\"   Has duplicate indices: {modeling_data.index.duplicated().any()}\")\n",
        "\n",
        "# Simple method: just reset index and copy\n",
        "print(\"\\n  Creating clean copy...\")\n",
        "\n",
        "modeling_data_clean = modeling_data.copy()\n",
        "modeling_data_clean.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Replace original\n",
        "modeling_data = modeling_data_clean\n",
        "\n",
        "print(f\"\\n Clean copy created:\")\n",
        "print(f\"   Shape: {modeling_data.shape}\")\n",
        "print(f\"   Has duplicate indices: {modeling_data.index.duplicated().any()}\")\n",
        "print(f\"   Index is unique: {modeling_data.index.is_unique}\")\n",
        "\n",
        "# Fix all other dataframes\n",
        "print(\"\\n Fixing indices in other dataframes...\")\n",
        "\n",
        "# Fix transactions\n",
        "print(\"   â€¢ transactions...\")\n",
        "transactions = transactions.copy()\n",
        "transactions.reset_index(drop=True, inplace=True)\n",
        "print(f\"     âœ“ Fixed. Has duplicates: {transactions.index.duplicated().any()}\")\n",
        "\n",
        "# Fix offers_completed\n",
        "print(\"   â€¢ offers_completed...\")\n",
        "offers_completed = offers_completed.copy()\n",
        "offers_completed.reset_index(drop=True, inplace=True)\n",
        "print(f\"     âœ“ Fixed. Has duplicates: {offers_completed.index.duplicated().any()}\")\n",
        "\n",
        "# Fix offers_viewed\n",
        "print(\"   â€¢ offers_viewed...\")\n",
        "offers_viewed = offers_viewed.copy()\n",
        "offers_viewed.reset_index(drop=True, inplace=True)\n",
        "print(f\"     âœ“ Fixed. Has duplicates: {offers_viewed.index.duplicated().any()}\")\n",
        "\n",
        "# Fix offers_received\n",
        "print(\"   â€¢ offers_received...\")\n",
        "offers_received = offers_received.copy()\n",
        "offers_received.reset_index(drop=True, inplace=True)\n",
        "print(f\"     âœ“ Fixed. Has duplicates: {offers_received.index.duplicated().any()}\")\n",
        "\n",
        "print(\"\\n ALL DATAFRAMES ARE NOW CLEAN AND INDEX-FREE!\")\n",
        "\n",
        "# Verify\n",
        "print(\"\\n Final verification:\")\n",
        "print(f\"   modeling_data: {modeling_data.shape}, unique index: {modeling_data.index.is_unique}\")\n",
        "print(f\"   transactions: {transactions.shape}, unique index: {transactions.index.is_unique}\")\n",
        "print(f\"   offers_completed: {offers_completed.shape}, unique index: {offers_completed.index.is_unique}\")\n",
        "print(f\"   offers_viewed: {offers_viewed.shape}, unique index: {offers_viewed.index.is_unique}\")\n",
        "print(f\"   offers_received: {offers_received.shape}, unique index: {offers_received.index.is_unique}\")\n",
        "\n",
        "print(\"\\n Ready to calculate outcomes!\")"
      ],
      "metadata": {
        "id": "Pd0E5zMyLAIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create helper functions"
      ],
      "metadata": {
        "id": "BKgxoOdRXMcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "COMPLETELY NEW APPROACH - Simple iteration without any fancy pandas operations\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CREATING OUTCOME CALCULATION FUNCTIONS (NEW SIMPLE APPROACH)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def calculate_conversion_outcome_simple(customer_id, decision_time, offer_id,\n",
        "                                        window_days, transactions_df,\n",
        "                                        offers_completed_df):\n",
        "    \"\"\"\n",
        "    Simple approach - just loop through dataframes\n",
        "    \"\"\"\n",
        "    window_end = decision_time + pd.Timedelta(days=window_days)\n",
        "\n",
        "    # Check column name\n",
        "    offer_col = 'offer_id' if 'offer_id' in offers_completed_df.columns else 'offer id'\n",
        "\n",
        "    # Count completed offers by looping\n",
        "    num_completed = 0\n",
        "    for idx in range(len(offers_completed_df)):\n",
        "        row = offers_completed_df.iloc[idx]\n",
        "        if (row['customer_id'] == customer_id and\n",
        "            str(row[offer_col]) == str(offer_id) and\n",
        "            row['timestamp'] >= decision_time and\n",
        "            row['timestamp'] <= window_end):\n",
        "            num_completed += 1\n",
        "\n",
        "    # Count transactions by looping\n",
        "    num_txns = 0\n",
        "    total_spent = 0.0\n",
        "    for idx in range(len(transactions_df)):\n",
        "        row = transactions_df.iloc[idx]\n",
        "        if (row['customer_id'] == customer_id and\n",
        "            row['timestamp'] >= decision_time and\n",
        "            row['timestamp'] <= window_end):\n",
        "            num_txns += 1\n",
        "            total_spent += float(row['amount'])\n",
        "\n",
        "    # Calculate outcomes\n",
        "    offer_completed = 1 if num_completed > 0 else 0\n",
        "    any_transaction = 1 if num_txns > 0 else 0\n",
        "    conversion = 1 if (offer_completed or any_transaction) else 0\n",
        "\n",
        "    return {\n",
        "        'offer_completed': offer_completed,\n",
        "        'any_transaction': any_transaction,\n",
        "        'conversion': conversion,\n",
        "        'num_transactions_in_window': num_txns,\n",
        "        'total_spent_in_window': total_spent,\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_retention_outcome_simple(customer_id, decision_time, window_days,\n",
        "                                       transactions_df):\n",
        "    \"\"\"\n",
        "    Simple approach - just loop through dataframe\n",
        "    \"\"\"\n",
        "    window_end = decision_time + pd.Timedelta(days=window_days)\n",
        "\n",
        "    # Count transactions by looping\n",
        "    num_txns = 0\n",
        "    total_spent = 0.0\n",
        "    for idx in range(len(transactions_df)):\n",
        "        row = transactions_df.iloc[idx]\n",
        "        if (row['customer_id'] == customer_id and\n",
        "            row['timestamp'] >= decision_time and\n",
        "            row['timestamp'] <= window_end):\n",
        "            num_txns += 1\n",
        "            total_spent += float(row['amount'])\n",
        "\n",
        "    # Calculate outcomes\n",
        "    is_active = 1 if num_txns > 0 else 0\n",
        "    avg_spent = total_spent / num_txns if num_txns > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'retention_active': is_active,\n",
        "        'retention_num_transactions': num_txns,\n",
        "        'retention_total_spent': total_spent,\n",
        "        'retention_avg_spent': avg_spent,\n",
        "    }\n",
        "\n",
        "print(\" Simple outcome calculation functions created!\")\n",
        "print(\"   (These use basic loops - slower but 100% reliable)\")\n",
        "\n",
        "# Test\n",
        "print(\"\\n Testing functions...\")\n",
        "try:\n",
        "    test_conv = calculate_conversion_outcome_simple(\n",
        "        customer_id=modeling_data['customer_id'].iloc[0],\n",
        "        decision_time=modeling_data['timestamp'].iloc[0],\n",
        "        offer_id=modeling_data['offer_id'].iloc[0],\n",
        "        window_days=14,\n",
        "        transactions_df=transactions,\n",
        "        offers_completed_df=offers_completed\n",
        "    )\n",
        "    print(f\" Conversion test passed! Result: {test_conv}\")\n",
        "\n",
        "    test_ret = calculate_retention_outcome_simple(\n",
        "        customer_id=modeling_data['customer_id'].iloc[0],\n",
        "        decision_time=modeling_data['timestamp'].iloc[0],\n",
        "        window_days=30,\n",
        "        transactions_df=transactions\n",
        "    )\n",
        "    print(f\" Retention test passed! Result: {test_ret}\")\n",
        "\n",
        "    print(\"\\n BOTH TESTS PASSED! Functions are ready!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Test failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "za2q94eUSaiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate conversion outcome"
      ],
      "metadata": {
        "id": "t3p8GQOaXRXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate conversion outcomes - OPTIMIZED VERSION\n",
        "Pre-filter dataframes by customer to make it much faster\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CALCULATING CONVERSION OUTCOMES (14-DAY WINDOW) - OPTIMIZED\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n Processing {len(modeling_data):,} offer instances...\")\n",
        "print(\"   Using optimized approach - should take ~3-5 minutes\\n\")\n",
        "\n",
        "# Pre-group data by customer_id for faster lookup\n",
        "print(\" Pre-grouping data by customer...\")\n",
        "transactions_by_customer = transactions.groupby('customer_id')\n",
        "offers_completed_by_customer = offers_completed.groupby('customer_id')\n",
        "\n",
        "# Check column name\n",
        "offer_col = 'offer_id' if 'offer_id' in offers_completed.columns else 'offer id'\n",
        "\n",
        "print(\" Pre-grouping complete! Starting calculations...\\n\")\n",
        "\n",
        "# Lists to store results\n",
        "results_offer_completed = []\n",
        "results_any_transaction = []\n",
        "results_conversion = []\n",
        "results_num_transactions = []\n",
        "results_total_spent = []\n",
        "\n",
        "# Process each row\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(len(modeling_data)), desc=\"Conversion outcomes\"):\n",
        "    customer_id = modeling_data['customer_id'].iloc[i]\n",
        "    decision_time = modeling_data['timestamp'].iloc[i]\n",
        "    offer_id = str(modeling_data['offer_id'].iloc[i])\n",
        "    window_end = decision_time + pd.Timedelta(days=CONVERSION_WINDOW_DAYS)\n",
        "\n",
        "    # Get this customer's data only\n",
        "    try:\n",
        "        customer_offers = offers_completed_by_customer.get_group(customer_id)\n",
        "    except KeyError:\n",
        "        customer_offers = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        customer_txns = transactions_by_customer.get_group(customer_id)\n",
        "    except KeyError:\n",
        "        customer_txns = pd.DataFrame()\n",
        "\n",
        "    # Count completed offers (much smaller dataset now)\n",
        "    num_completed = 0\n",
        "    if len(customer_offers) > 0:\n",
        "        for idx in range(len(customer_offers)):\n",
        "            row = customer_offers.iloc[idx]\n",
        "            if (str(row[offer_col]) == offer_id and\n",
        "                row['timestamp'] >= decision_time and\n",
        "                row['timestamp'] <= window_end):\n",
        "                num_completed += 1\n",
        "\n",
        "    # Count transactions (much smaller dataset now)\n",
        "    num_txns = 0\n",
        "    total_spent = 0.0\n",
        "    if len(customer_txns) > 0:\n",
        "        for idx in range(len(customer_txns)):\n",
        "            row = customer_txns.iloc[idx]\n",
        "            if (row['timestamp'] >= decision_time and\n",
        "                row['timestamp'] <= window_end):\n",
        "                num_txns += 1\n",
        "                total_spent += float(row['amount'])\n",
        "\n",
        "    # Calculate outcomes\n",
        "    offer_completed = 1 if num_completed > 0 else 0\n",
        "    any_transaction = 1 if num_txns > 0 else 0\n",
        "    conversion = 1 if (offer_completed or any_transaction) else 0\n",
        "\n",
        "    results_offer_completed.append(offer_completed)\n",
        "    results_any_transaction.append(any_transaction)\n",
        "    results_conversion.append(conversion)\n",
        "    results_num_transactions.append(num_txns)\n",
        "    results_total_spent.append(total_spent)\n",
        "\n",
        "# Add results as new columns\n",
        "modeling_data['conv_14d_offer_completed'] = results_offer_completed\n",
        "modeling_data['conv_14d_any_transaction'] = results_any_transaction\n",
        "modeling_data['conv_14d_conversion'] = results_conversion\n",
        "modeling_data['conv_14d_num_transactions_in_window'] = results_num_transactions\n",
        "modeling_data['conv_14d_total_spent_in_window'] = results_total_spent\n",
        "\n",
        "print(f\"\\n Conversion outcomes calculated!\")\n",
        "print(f\"   Total columns now: {modeling_data.shape[1]}\")\n",
        "\n",
        "# Statistics\n",
        "print(\"\\n Conversion Outcome Statistics:\")\n",
        "conv_cols = ['conv_14d_offer_completed', 'conv_14d_any_transaction',\n",
        "             'conv_14d_conversion', 'conv_14d_num_transactions_in_window',\n",
        "             'conv_14d_total_spent_in_window']\n",
        "print(modeling_data[conv_cols].describe())\n",
        "\n",
        "# Conversion rate\n",
        "conv_rate = modeling_data['conv_14d_conversion'].mean() * 100\n",
        "print(f\"\\n Overall 14-day conversion rate: {conv_rate:.2f}%\")\n",
        "\n",
        "# Sample\n",
        "print(\"\\n Sample of conversion outcomes:\")\n",
        "display(modeling_data[['customer_id', 'timestamp', 'offer_type_simple'] + conv_cols].head(10))"
      ],
      "metadata": {
        "id": "ZusAItroWZE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Retention Outcomes"
      ],
      "metadata": {
        "id": "5_9AuMwGXYkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate retention outcomes - OPTIMIZED VERSION\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CALCULATING RETENTION OUTCOMES (30-DAY WINDOW) - OPTIMIZED\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n Processing {len(modeling_data):,} offer instances...\")\n",
        "print(\"   Using optimized approach - should take ~2-3 minutes\\n\")\n",
        "\n",
        "# Use the same pre-grouped data\n",
        "# transactions_by_customer is already created in previous cell\n",
        "\n",
        "# Lists to store results\n",
        "results_active = []\n",
        "results_num_transactions = []\n",
        "results_total_spent = []\n",
        "results_avg_spent = []\n",
        "\n",
        "# Process each row\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(range(len(modeling_data)), desc=\"Retention outcomes\"):\n",
        "    customer_id = modeling_data['customer_id'].iloc[i]\n",
        "    decision_time = modeling_data['timestamp'].iloc[i]\n",
        "    window_end = decision_time + pd.Timedelta(days=RETENTION_WINDOW_DAYS)\n",
        "\n",
        "    # Get this customer's transactions only\n",
        "    try:\n",
        "        customer_txns = transactions_by_customer.get_group(customer_id)\n",
        "    except KeyError:\n",
        "        customer_txns = pd.DataFrame()\n",
        "\n",
        "    # Count transactions\n",
        "    num_txns = 0\n",
        "    total_spent = 0.0\n",
        "    if len(customer_txns) > 0:\n",
        "        for idx in range(len(customer_txns)):\n",
        "            row = customer_txns.iloc[idx]\n",
        "            if (row['timestamp'] >= decision_time and\n",
        "                row['timestamp'] <= window_end):\n",
        "                num_txns += 1\n",
        "                total_spent += float(row['amount'])\n",
        "\n",
        "    # Calculate outcomes\n",
        "    is_active = 1 if num_txns > 0 else 0\n",
        "    avg_spent = total_spent / num_txns if num_txns > 0 else 0.0\n",
        "\n",
        "    results_active.append(is_active)\n",
        "    results_num_transactions.append(num_txns)\n",
        "    results_total_spent.append(total_spent)\n",
        "    results_avg_spent.append(avg_spent)\n",
        "\n",
        "# Add results as new columns\n",
        "modeling_data['ret_30d_retention_active'] = results_active\n",
        "modeling_data['ret_30d_retention_num_transactions'] = results_num_transactions\n",
        "modeling_data['ret_30d_retention_total_spent'] = results_total_spent\n",
        "modeling_data['ret_30d_retention_avg_spent'] = results_avg_spent\n",
        "\n",
        "print(f\"\\n Retention outcomes calculated!\")\n",
        "print(f\"   Total columns now: {modeling_data.shape[1]}\")\n",
        "\n",
        "# Statistics\n",
        "print(\"\\n Retention Outcome Statistics:\")\n",
        "ret_cols = ['ret_30d_retention_active', 'ret_30d_retention_num_transactions',\n",
        "            'ret_30d_retention_total_spent', 'ret_30d_retention_avg_spent']\n",
        "print(modeling_data[ret_cols].describe())\n",
        "\n",
        "# Retention rate\n",
        "ret_rate = modeling_data['ret_30d_retention_active'].mean() * 100\n",
        "print(f\"\\n Overall 30-day retention rate: {ret_rate:.2f}%\")\n",
        "\n",
        "# Sample\n",
        "print(\"\\n Sample of retention outcomes:\")\n",
        "display(modeling_data[['customer_id', 'timestamp', 'offer_type_simple'] + ret_cols].head(10))"
      ],
      "metadata": {
        "id": "1W_pDvsBWmZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze Outcomes by Offer Type"
      ],
      "metadata": {
        "id": "XQprtL1dYAwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze conversion and retention by offer type\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" OUTCOME ANALYSIS BY OFFER TYPE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Group by offer type\n",
        "outcome_by_offer = modeling_data.groupby('offer_type_simple').agg({\n",
        "    'conv_14d_conversion': ['count', 'mean'],\n",
        "    'conv_14d_offer_completed': 'mean',\n",
        "    'conv_14d_any_transaction': 'mean',\n",
        "    'conv_14d_total_spent_in_window': 'mean',\n",
        "    'ret_30d_retention_active': 'mean',\n",
        "    'ret_30d_retention_num_transactions': 'mean',\n",
        "    'ret_30d_retention_total_spent': 'mean',\n",
        "}).round(4)\n",
        "\n",
        "outcome_by_offer.columns = ['_'.join(col).strip() for col in outcome_by_offer.columns.values]\n",
        "outcome_by_offer = outcome_by_offer.rename(columns={\n",
        "    'conv_14d_conversion_count': 'num_instances',\n",
        "    'conv_14d_conversion_mean': 'conversion_rate',\n",
        "    'conv_14d_offer_completed_mean': 'offer_completion_rate',\n",
        "    'conv_14d_any_transaction_mean': 'transaction_rate',\n",
        "    'conv_14d_total_spent_in_window_mean': 'avg_revenue_14d',\n",
        "    'ret_30d_retention_active_mean': 'retention_rate',\n",
        "    'ret_30d_retention_num_transactions_mean': 'avg_transactions_30d',\n",
        "    'ret_30d_retention_total_spent_mean': 'avg_revenue_30d',\n",
        "})\n",
        "\n",
        "print(\"\\n Outcomes by Offer Type:\")\n",
        "display(outcome_by_offer)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# Conversion rate by offer type\n",
        "outcome_by_offer['conversion_rate'].plot(\n",
        "    kind='bar', ax=axes[0, 0], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black'\n",
        ")\n",
        "axes[0, 0].set_title('Conversion Rate by Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Conversion Rate')\n",
        "axes[0, 0].set_xlabel('')\n",
        "axes[0, 0].tick_params(axis='x', rotation=0)\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "axes[0, 0].axhline(modeling_data['conv_14d_conversion'].mean(), color='red',\n",
        "                   linestyle='--', label='Overall Avg')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Offer completion rate by offer type\n",
        "outcome_by_offer['offer_completion_rate'].plot(\n",
        "    kind='bar', ax=axes[0, 1], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black'\n",
        ")\n",
        "axes[0, 1].set_title('Offer Completion Rate by Type', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Completion Rate')\n",
        "axes[0, 1].set_xlabel('')\n",
        "axes[0, 1].tick_params(axis='x', rotation=0)\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Average revenue (14d) by offer type\n",
        "outcome_by_offer['avg_revenue_14d'].plot(\n",
        "    kind='bar', ax=axes[0, 2], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black'\n",
        ")\n",
        "axes[0, 2].set_title('Avg Revenue (14d) by Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].set_ylabel('Average Revenue ($)')\n",
        "axes[0, 2].set_xlabel('')\n",
        "axes[0, 2].tick_params(axis='x', rotation=0)\n",
        "axes[0, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Retention rate by offer type\n",
        "outcome_by_offer['retention_rate'].plot(\n",
        "    kind='bar', ax=axes[1, 0], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black'\n",
        ")\n",
        "axes[1, 0].set_title('Retention Rate (30d) by Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Retention Rate')\n",
        "axes[1, 0].set_xlabel('')\n",
        "axes[1, 0].tick_params(axis='x', rotation=0)\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "axes[1, 0].axhline(modeling_data['ret_30d_retention_active'].mean(), color='red',\n",
        "                   linestyle='--', label='Overall Avg')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Avg transactions (30d) by offer type\n",
        "outcome_by_offer['avg_transactions_30d'].plot(\n",
        "    kind='bar', ax=axes[1, 1], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black'\n",
        ")\n",
        "axes[1, 1].set_title('Avg Transactions (30d) by Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Avg # Transactions')\n",
        "axes[1, 1].set_xlabel('')\n",
        "axes[1, 1].tick_params(axis='x', rotation=0)\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Avg revenue (30d) by offer type\n",
        "outcome_by_offer['avg_revenue_30d'].plot(\n",
        "    kind='bar', ax=axes[1, 2], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black'\n",
        ")\n",
        "axes[1, 2].set_title('Avg Revenue (30d) by Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_ylabel('Average Revenue ($)')\n",
        "axes[1, 2].set_xlabel('')\n",
        "axes[1, 2].tick_params(axis='x', rotation=0)\n",
        "axes[1, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Outcome analysis complete!\")"
      ],
      "metadata": {
        "id": "apD0x0gJX7Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Additional Derived Features"
      ],
      "metadata": {
        "id": "KPNvRLizYFrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create additional derived features based on existing features\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ”§ CREATING DERIVED FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Customer value score (combines spending and frequency)\n",
        "modeling_data['customer_value_score'] = (\n",
        "    modeling_data['rfm_lifetime_monetary_total'] *\n",
        "    np.log1p(modeling_data['rfm_lifetime_frequency'])\n",
        ")\n",
        "\n",
        "# Engagement score (combines offer interactions)\n",
        "modeling_data['engagement_score'] = (\n",
        "    modeling_data['prev_view_rate'] * 0.5 +\n",
        "    modeling_data['prev_completion_rate'] * 0.5\n",
        ")\n",
        "\n",
        "# Recent activity flag (transacted in last 7 days)\n",
        "modeling_data['is_recently_active'] = (\n",
        "    modeling_data['rfm_7d_frequency'] > 0\n",
        ").astype(int)\n",
        "\n",
        "# High spender flag (above median spending)\n",
        "median_lifetime_spend = modeling_data['rfm_lifetime_monetary_total'].median()\n",
        "modeling_data['is_high_spender'] = (\n",
        "    modeling_data['rfm_lifetime_monetary_total'] > median_lifetime_spend\n",
        ").astype(int)\n",
        "\n",
        "# Offer experience (number of previous offers received)\n",
        "modeling_data['offer_experience_level'] = pd.cut(\n",
        "    modeling_data['prev_offers_received'],\n",
        "    bins=[-1, 0, 2, 5, 100],\n",
        "    labels=['new', 'low', 'medium', 'high']\n",
        ")\n",
        "\n",
        "# Age group\n",
        "modeling_data['age_group'] = pd.cut(\n",
        "    modeling_data['age_clean'],\n",
        "    bins=[0, 30, 45, 60, 100],\n",
        "    labels=['young', 'middle', 'senior', 'elderly']\n",
        ")\n",
        "\n",
        "# Income group\n",
        "modeling_data['income_group'] = pd.cut(\n",
        "    modeling_data['income_clean'],\n",
        "    bins=[0, 50000, 75000, 100000, 200000],\n",
        "    labels=['low', 'medium', 'high', 'very_high']\n",
        ")\n",
        "\n",
        "print(\" Derived features created:\")\n",
        "derived_features = ['customer_value_score', 'engagement_score', 'is_recently_active',\n",
        "                    'is_high_spender', 'offer_experience_level', 'age_group', 'income_group']\n",
        "for feat in derived_features:\n",
        "    print(f\"   â€¢ {feat}\")\n",
        "\n",
        "print(f\"\\n   Total columns now: {modeling_data.shape[1]}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n Sample of derived features:\")\n",
        "display(modeling_data[['customer_id'] + derived_features].head(10))\n",
        "\n",
        "# Show statistics\n",
        "print(\"\\n Derived feature statistics:\")\n",
        "print(modeling_data[['customer_value_score', 'engagement_score']].describe())"
      ],
      "metadata": {
        "id": "2VH4RL_jYIAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Data Quality Check and Save"
      ],
      "metadata": {
        "id": "gcEUHrERYL-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Final data quality check and save the modeling dataset\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ” FINAL DATA QUALITY CHECK\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n Final Dataset Shape: {modeling_data.shape}\")\n",
        "print(f\"   Rows: {modeling_data.shape[0]:,}\")\n",
        "print(f\"   Columns: {modeling_data.shape[1]}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n Missing Values Check:\")\n",
        "missing_summary = modeling_data.isnull().sum()\n",
        "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
        "\n",
        "if len(missing_summary) > 0:\n",
        "    print(f\"\\nâš ï¸  Columns with missing values:\")\n",
        "    print(missing_summary.head(20))\n",
        "    print(f\"\\n   Missing percentage:\")\n",
        "    print((missing_summary / len(modeling_data) * 100).round(2).head(20))\n",
        "else:\n",
        "    print(\" No missing values!\")\n",
        "\n",
        "# Check target variable distribution\n",
        "print(\"\\nðŸŽ¯ Target Variable Distribution:\")\n",
        "print(\"\\nConversion (14d):\")\n",
        "print(modeling_data['conv_14d_conversion'].value_counts())\n",
        "print(f\"Conversion rate: {modeling_data['conv_14d_conversion'].mean()*100:.2f}%\")\n",
        "\n",
        "print(\"\\nRetention (30d):\")\n",
        "print(modeling_data['ret_30d_retention_active'].value_counts())\n",
        "print(f\"Retention rate: {modeling_data['ret_30d_retention_active'].mean()*100:.2f}%\")\n",
        "\n",
        "# Memory usage\n",
        "print(f\"\\n Memory Usage: {modeling_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Save the modeling dataset\n",
        "print(\"\\n SAVING MODELING DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "modeling_data.to_csv('cleaned_data/modeling_dataset_complete.csv', index=False)\n",
        "print(\" Saved: modeling_dataset_complete.csv\")\n",
        "\n",
        "# Also save a pickle file for faster loading\n",
        "modeling_data.to_pickle('cleaned_data/modeling_dataset_complete.pkl')\n",
        "print(\" Saved: modeling_dataset_complete.pkl\")\n",
        "\n",
        "print(f\"\\n Modeling dataset saved successfully!\")\n",
        "print(f\"   Location: cleaned_data/\")\n",
        "print(f\"   Files: modeling_dataset_complete.csv and .pkl\")"
      ],
      "metadata": {
        "id": "6s_oK6fQYOB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select features for clustering"
      ],
      "metadata": {
        "id": "Hxjz2sx6Yotn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Select and prepare features for clustering\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SELECTING FEATURES FOR CLUSTERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select clustering features - focusing on customer behavior\n",
        "clustering_features = [\n",
        "    # RFM features (most important)\n",
        "    'rfm_lifetime_recency_days',\n",
        "    'rfm_lifetime_frequency',\n",
        "    'rfm_lifetime_monetary_total',\n",
        "    'rfm_lifetime_monetary_mean',\n",
        "\n",
        "    # Recent behavior\n",
        "    'rfm_30d_frequency',\n",
        "    'rfm_30d_monetary_total',\n",
        "\n",
        "    # Offer engagement\n",
        "    'prev_offers_received',\n",
        "    'prev_view_rate',\n",
        "    'prev_completion_rate',\n",
        "\n",
        "    # Demographics\n",
        "    'age_clean',\n",
        "    'income_clean',\n",
        "    'member_tenure_days',\n",
        "]\n",
        "\n",
        "print(f\"\\n Selected {len(clustering_features)} features for clustering:\")\n",
        "for i, feat in enumerate(clustering_features, 1):\n",
        "    print(f\"   {i}. {feat}\")\n",
        "\n",
        "# Create clustering dataset (one row per customer, not per offer)\n",
        "print(\"\\n Aggregating to customer level...\")\n",
        "\n",
        "# Get unique customers with their average characteristics\n",
        "customer_level_data = modeling_data.groupby('customer_id').agg({\n",
        "    'rfm_lifetime_recency_days': 'first',  # These are constant per customer\n",
        "    'rfm_lifetime_frequency': 'first',\n",
        "    'rfm_lifetime_monetary_total': 'first',\n",
        "    'rfm_lifetime_monetary_mean': 'first',\n",
        "    'rfm_30d_frequency': 'first',\n",
        "    'rfm_30d_monetary_total': 'first',\n",
        "    'prev_offers_received': 'first',\n",
        "    'prev_view_rate': 'first',\n",
        "    'prev_completion_rate': 'first',\n",
        "    'age_clean': 'first',\n",
        "    'income_clean': 'first',\n",
        "    'member_tenure_days': 'first',\n",
        "    'gender': 'first',\n",
        "    'offer_type_simple': 'first',  # Keep for analysis\n",
        "    'conv_14d_conversion': 'mean',  # Average conversion rate\n",
        "    'ret_30d_retention_active': 'mean',  # Average retention\n",
        "}).reset_index()\n",
        "\n",
        "print(f\" Customer-level dataset created:\")\n",
        "print(f\"   Shape: {customer_level_data.shape}\")\n",
        "print(f\"   Unique customers: {len(customer_level_data):,}\")\n",
        "\n",
        "# Handle missing values\n",
        "print(\"\\n Handling missing values...\")\n",
        "print(f\"Missing values before:\")\n",
        "print(customer_level_data[clustering_features].isnull().sum())\n",
        "\n",
        "# Fill missing values with median\n",
        "for col in clustering_features:\n",
        "    if customer_level_data[col].isnull().any():\n",
        "        median_val = customer_level_data[col].median()\n",
        "        customer_level_data[col].fillna(median_val, inplace=True)\n",
        "\n",
        "print(f\"\\n Missing values after:\")\n",
        "print(customer_level_data[clustering_features].isnull().sum())\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n Sample of customer-level data:\")\n",
        "display(customer_level_data[['customer_id'] + clustering_features[:6]].head(10))"
      ],
      "metadata": {
        "id": "6RsYKWdhYcg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale Features"
      ],
      "metadata": {
        "id": "mGwvsNpIYtV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Scale features for clustering\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"  SCALING FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Extract feature matrix\n",
        "X = customer_level_data[clustering_features].values\n",
        "\n",
        "print(f\"\\n Feature matrix shape: {X.shape}\")\n",
        "print(f\"   Samples (customers): {X.shape[0]:,}\")\n",
        "print(f\"   Features: {X.shape[1]}\")\n",
        "\n",
        "# Show feature statistics before scaling\n",
        "print(\"\\n Feature statistics BEFORE scaling:\")\n",
        "print(pd.DataFrame(X, columns=clustering_features).describe())\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"\\n Features scaled using StandardScaler (mean=0, std=1)\")\n",
        "\n",
        "# Show feature statistics after scaling\n",
        "print(\"\\n Feature statistics AFTER scaling:\")\n",
        "print(pd.DataFrame(X_scaled, columns=clustering_features).describe())\n",
        "\n",
        "# Save scaled data\n",
        "customer_level_data_scaled = customer_level_data.copy()\n",
        "for i, col in enumerate(clustering_features):\n",
        "    customer_level_data_scaled[f'{col}_scaled'] = X_scaled[:, i]\n",
        "\n",
        "print(f\"\\n Scaled features ready for clustering!\")"
      ],
      "metadata": {
        "id": "7uNtEY5tYnhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine Optimal Number of Clusters"
      ],
      "metadata": {
        "id": "RlRNtN97Yy2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Determine optimal number of clusters using Elbow Method and Silhouette Score\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" DETERMINING OPTIMAL NUMBER OF CLUSTERS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "print(\"\\n Testing different numbers of clusters (2-10)...\")\n",
        "print(\"   This will take a minute...\\n\")\n",
        "\n",
        "# Test different numbers of clusters\n",
        "k_range = range(2, 11)\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    print(f\"Testing k={k}...\", end=\" \")\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouette = silhouette_score(X_scaled, kmeans.labels_)\n",
        "    silhouette_scores.append(silhouette)\n",
        "    print(f\"Inertia: {kmeans.inertia_:.2f}, Silhouette: {silhouette:.3f}\")\n",
        "\n",
        "print(\"\\n Testing complete!\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Elbow plot\n",
        "axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "axes[0].set_ylabel('Inertia (Within-cluster sum of squares)', fontsize=12)\n",
        "axes[0].set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xticks(k_range)\n",
        "\n",
        "# Silhouette plot\n",
        "axes[1].plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
        "axes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
        "axes[1].set_title('Silhouette Score', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xticks(k_range)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best k (highest silhouette score)\n",
        "best_k_idx = np.argmax(silhouette_scores)\n",
        "best_k = k_range[best_k_idx]\n",
        "\n",
        "print(f\"\\n RESULTS:\")\n",
        "print(f\"   Best k (by Silhouette Score): {best_k}\")\n",
        "print(f\"   Silhouette Score: {silhouette_scores[best_k_idx]:.3f}\")\n",
        "\n",
        "print(\"\\n RECOMMENDATION:\")\n",
        "print(f\"   Based on the analysis, k={best_k} clusters is optimal.\")\n",
        "print(f\"   However, for interpretability, we'll use k=4 or k=5\")\n",
        "print(f\"   (Common practice: 3-5 segments are easier to act on)\")\n",
        "\n",
        "# Let's use k=4 for good interpretability\n",
        "optimal_k = 4\n",
        "print(f\"\\n We'll proceed with k={optimal_k} clusters\")"
      ],
      "metadata": {
        "id": "rxpaF9HWYwKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply K-Means Clustering"
      ],
      "metadata": {
        "id": "1Ewij1O8Y32W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Apply K-Means clustering with optimal k\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" APPLYING K-MEANS CLUSTERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "optimal_k = 4  # Can adjust this based on previous cell results\n",
        "\n",
        "print(f\"\\n Fitting K-Means with k={optimal_k} clusters...\")\n",
        "\n",
        "# Fit K-Means\n",
        "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=500)\n",
        "customer_level_data['segment'] = kmeans_final.fit_predict(X_scaled)\n",
        "\n",
        "print(f\" Clustering complete!\")\n",
        "\n",
        "# Calculate metrics\n",
        "inertia = kmeans_final.inertia_\n",
        "silhouette = silhouette_score(X_scaled, customer_level_data['segment'])\n",
        "\n",
        "print(f\"\\n Clustering Metrics:\")\n",
        "print(f\"   Inertia: {inertia:.2f}\")\n",
        "print(f\"   Silhouette Score: {silhouette:.3f}\")\n",
        "\n",
        "# Segment sizes\n",
        "print(f\"\\n Segment Distribution:\")\n",
        "segment_counts = customer_level_data['segment'].value_counts().sort_index()\n",
        "for seg, count in segment_counts.items():\n",
        "    pct = (count / len(customer_level_data)) * 100\n",
        "    print(f\"   Segment {seg}: {count:,} customers ({pct:.1f}%)\")\n",
        "\n",
        "# Visualize segment distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "segment_counts.plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#F7DC6F'],\n",
        "                    edgecolor='black')\n",
        "ax.set_title('Customer Segment Distribution', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Segment', fontsize=12)\n",
        "ax.set_ylabel('Number of Customers', fontsize=12)\n",
        "ax.tick_params(axis='x', rotation=0)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add count labels on bars\n",
        "for i, v in enumerate(segment_counts):\n",
        "    ax.text(i, v + 50, f'{v:,}\\n({v/len(customer_level_data)*100:.1f}%)',\n",
        "            ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Segments created successfully!\")"
      ],
      "metadata": {
        "id": "j48hnnfnY0lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segment Profiling"
      ],
      "metadata": {
        "id": "xLXGWyJaZG68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Profile each segment - calculate average characteristics\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SEGMENT PROFILING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate segment profiles\n",
        "segment_profiles = customer_level_data.groupby('segment').agg({\n",
        "    # RFM\n",
        "    'rfm_lifetime_recency_days': 'mean',\n",
        "    'rfm_lifetime_frequency': 'mean',\n",
        "    'rfm_lifetime_monetary_total': 'mean',\n",
        "    'rfm_lifetime_monetary_mean': 'mean',\n",
        "\n",
        "    # Recent behavior\n",
        "    'rfm_30d_frequency': 'mean',\n",
        "    'rfm_30d_monetary_total': 'mean',\n",
        "\n",
        "    # Engagement\n",
        "    'prev_offers_received': 'mean',\n",
        "    'prev_view_rate': 'mean',\n",
        "    'prev_completion_rate': 'mean',\n",
        "\n",
        "    # Demographics\n",
        "    'age_clean': 'mean',\n",
        "    'income_clean': 'mean',\n",
        "    'member_tenure_days': 'mean',\n",
        "\n",
        "    # Outcomes\n",
        "    'conv_14d_conversion': 'mean',\n",
        "    'ret_30d_retention_active': 'mean',\n",
        "\n",
        "    # Count\n",
        "    'customer_id': 'count'\n",
        "}).round(2)\n",
        "\n",
        "segment_profiles = segment_profiles.rename(columns={'customer_id': 'customer_count'})\n",
        "\n",
        "print(\"\\n SEGMENT PROFILES:\")\n",
        "print(\"=\" * 80)\n",
        "display(segment_profiles)\n",
        "\n",
        "# Transpose for better readability\n",
        "print(\"\\n SEGMENT PROFILES (Transposed):\")\n",
        "print(\"=\" * 80)\n",
        "display(segment_profiles.T)"
      ],
      "metadata": {
        "id": "Uhs1ee7wZBbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Visualize key segment characteristics\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" VISUALIZING SEGMENT CHARACTERISTICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
        "\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#F7DC6F']\n",
        "\n",
        "# 1. Average Frequency\n",
        "segment_profiles['rfm_lifetime_frequency'].plot(\n",
        "    kind='bar', ax=axes[0, 0], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[0, 0].set_title('Average Purchase Frequency', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Avg Transactions')\n",
        "axes[0, 0].set_xlabel('Segment')\n",
        "axes[0, 0].tick_params(axis='x', rotation=0)\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 2. Average Total Spend\n",
        "segment_profiles['rfm_lifetime_monetary_total'].plot(\n",
        "    kind='bar', ax=axes[0, 1], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[0, 1].set_title('Average Total Spend (Lifetime)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Avg Spend ($)')\n",
        "axes[0, 1].set_xlabel('Segment')\n",
        "axes[0, 1].tick_params(axis='x', rotation=0)\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 3. Average Recency\n",
        "segment_profiles['rfm_lifetime_recency_days'].plot(\n",
        "    kind='bar', ax=axes[0, 2], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[0, 2].set_title('Average Recency (Days Since Last Purchase)', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].set_ylabel('Days')\n",
        "axes[0, 2].set_xlabel('Segment')\n",
        "axes[0, 2].tick_params(axis='x', rotation=0)\n",
        "axes[0, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 4. Recent Activity (30d frequency)\n",
        "segment_profiles['rfm_30d_frequency'].plot(\n",
        "    kind='bar', ax=axes[1, 0], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[1, 0].set_title('Recent Activity (30d Transactions)', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Avg Transactions')\n",
        "axes[1, 0].set_xlabel('Segment')\n",
        "axes[1, 0].tick_params(axis='x', rotation=0)\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 5. Offer Engagement (view rate)\n",
        "segment_profiles['prev_view_rate'].plot(\n",
        "    kind='bar', ax=axes[1, 1], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[1, 1].set_title('Offer View Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('View Rate')\n",
        "axes[1, 1].set_xlabel('Segment')\n",
        "axes[1, 1].tick_params(axis='x', rotation=0)\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 6. Offer Completion Rate\n",
        "segment_profiles['prev_completion_rate'].plot(\n",
        "    kind='bar', ax=axes[1, 2], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[1, 2].set_title('Offer Completion Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_ylabel('Completion Rate')\n",
        "axes[1, 2].set_xlabel('Segment')\n",
        "axes[1, 2].tick_params(axis='x', rotation=0)\n",
        "axes[1, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 7. Conversion Rate\n",
        "segment_profiles['conv_14d_conversion'].plot(\n",
        "    kind='bar', ax=axes[2, 0], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[2, 0].set_title('Conversion Rate (14d)', fontsize=12, fontweight='bold')\n",
        "axes[2, 0].set_ylabel('Conversion Rate')\n",
        "axes[2, 0].set_xlabel('Segment')\n",
        "axes[2, 0].tick_params(axis='x', rotation=0)\n",
        "axes[2, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 8. Retention Rate\n",
        "segment_profiles['ret_30d_retention_active'].plot(\n",
        "    kind='bar', ax=axes[2, 1], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[2, 1].set_title('Retention Rate (30d)', fontsize=12, fontweight='bold')\n",
        "axes[2, 1].set_ylabel('Retention Rate')\n",
        "axes[2, 1].set_xlabel('Segment')\n",
        "axes[2, 1].tick_params(axis='x', rotation=0)\n",
        "axes[2, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 9. Average Income\n",
        "segment_profiles['income_clean'].plot(\n",
        "    kind='bar', ax=axes[2, 2], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[2, 2].set_title('Average Income', fontsize=12, fontweight='bold')\n",
        "axes[2, 2].set_ylabel('Income ($)')\n",
        "axes[2, 2].set_xlabel('Segment')\n",
        "axes[2, 2].tick_params(axis='x', rotation=0)\n",
        "axes[2, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Segment visualization complete!\")"
      ],
      "metadata": {
        "id": "Iq0RGeftZJDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Assign meaningful names to segments based on their characteristics\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"  NAMING SEGMENTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Analyze each segment to assign names\n",
        "print(\"\\n Analyzing segment characteristics...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for seg in range(optimal_k):\n",
        "    print(f\"\\n SEGMENT {seg}:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    seg_data = segment_profiles.loc[seg]\n",
        "\n",
        "    print(f\"Size: {int(seg_data['customer_count']):,} customers\")\n",
        "    print(f\"Frequency: {seg_data['rfm_lifetime_frequency']:.1f} transactions\")\n",
        "    print(f\"Total Spend: ${seg_data['rfm_lifetime_monetary_total']:.2f}\")\n",
        "    print(f\"Avg Transaction: ${seg_data['rfm_lifetime_monetary_mean']:.2f}\")\n",
        "    print(f\"Recency: {seg_data['rfm_lifetime_recency_days']:.0f} days\")\n",
        "    print(f\"Recent Activity (30d): {seg_data['rfm_30d_frequency']:.2f} txns\")\n",
        "    print(f\"Offer View Rate: {seg_data['prev_view_rate']:.1%}\")\n",
        "    print(f\"Offer Completion Rate: {seg_data['prev_completion_rate']:.1%}\")\n",
        "    print(f\"Conversion Rate: {seg_data['conv_14d_conversion']:.1%}\")\n",
        "    print(f\"Retention Rate: {seg_data['ret_30d_retention_active']:.1%}\")\n",
        "    print(f\"Avg Income: ${seg_data['income_clean']:,.0f}\")\n",
        "\n",
        "# Based on the profiles, assign names\n",
        "# You'll need to adjust these based on your actual results\n",
        "segment_names = {\n",
        "    0: \"Segment 0\",  # Will update based on characteristics\n",
        "    1: \"Segment 1\",\n",
        "    2: \"Segment 2\",\n",
        "    3: \"Segment 3\"\n",
        "}\n",
        "\n",
        "# Let me create a mapping based on common patterns\n",
        "# We'll determine this after seeing the results\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\" SEGMENT NAME SUGGESTIONS:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "Based on typical RFM segmentation patterns, segments usually fall into:\n",
        "\n",
        "1. \"Champions\" / \"Loyal Regulars\"\n",
        "   - High frequency, high spend, low recency\n",
        "   - Best customers, need retention focus\n",
        "\n",
        "2. \"Potential Loyalists\" / \"Promising\"\n",
        "   - Medium-high frequency, growing spend\n",
        "   - Target for upselling and loyalty programs\n",
        "\n",
        "3. \"At Risk\" / \"Needs Attention\"\n",
        "   - Previously active, now dormant (high recency)\n",
        "   - Target for win-back campaigns\n",
        "\n",
        "4. \"New/Low Value\" / \"Occasional\"\n",
        "   - Low frequency, low spend\n",
        "   - Target for activation and engagement\n",
        "\n",
        "Examine the profiles above and update the names accordingly.\n",
        "\"\"\")\n",
        "\n",
        "# For now, let's create descriptive labels based on spend and frequency\n",
        "segment_descriptions = {}\n",
        "for seg in range(optimal_k):\n",
        "    freq = segment_profiles.loc[seg, 'rfm_lifetime_frequency']\n",
        "    spend = segment_profiles.loc[seg, 'rfm_lifetime_monetary_total']\n",
        "    recency = segment_profiles.loc[seg, 'rfm_lifetime_recency_days']\n",
        "\n",
        "    if freq > segment_profiles['rfm_lifetime_frequency'].median() and spend > segment_profiles['rfm_lifetime_monetary_total'].median():\n",
        "        if recency < segment_profiles['rfm_lifetime_recency_days'].median():\n",
        "            label = f\"Champions (Seg {seg})\"\n",
        "        else:\n",
        "            label = f\"At Risk High Value (Seg {seg})\"\n",
        "    elif freq > segment_profiles['rfm_lifetime_frequency'].median():\n",
        "        label = f\"Frequent Buyers (Seg {seg})\"\n",
        "    elif spend > segment_profiles['rfm_lifetime_monetary_total'].median():\n",
        "        label = f\"Big Spenders (Seg {seg})\"\n",
        "    else:\n",
        "        label = f\"Casual Shoppers (Seg {seg})\"\n",
        "\n",
        "    segment_descriptions[seg] = label\n",
        "\n",
        "print(\"\\n  AUTO-GENERATED SEGMENT LABELS:\")\n",
        "print(\"=\" * 80)\n",
        "for seg, label in segment_descriptions.items():\n",
        "    print(f\"Segment {seg}: {label}\")\n",
        "\n",
        "# Add segment names to customer data\n",
        "customer_level_data['segment_name'] = customer_level_data['segment'].map(segment_descriptions)\n",
        "\n",
        "print(\"\\n Segments named!\")"
      ],
      "metadata": {
        "id": "0mXilVKAZMFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze segment demographics\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SEGMENT DEMOGRAPHICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Gender distribution by segment\n",
        "print(\"\\n Gender Distribution by Segment:\")\n",
        "gender_dist = pd.crosstab(\n",
        "    customer_level_data['segment_name'],\n",
        "    customer_level_data['gender'],\n",
        "    normalize='index'\n",
        ") * 100\n",
        "\n",
        "display(gender_dist.round(1))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Gender distribution\n",
        "gender_dist.plot(kind='bar', ax=axes[0], stacked=False,\n",
        "                color=['#3498db', '#e74c3c', '#95a5a6'], edgecolor='black')\n",
        "axes[0].set_title('Gender Distribution by Segment (%)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Percentage (%)')\n",
        "axes[0].set_xlabel('Segment')\n",
        "axes[0].legend(title='Gender')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Average age by segment\n",
        "avg_age = customer_level_data.groupby('segment_name')['age_clean'].mean()\n",
        "avg_age.plot(kind='bar', ax=axes[1], color='#9b59b6', edgecolor='black')\n",
        "axes[1].set_title('Average Age by Segment', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Average Age (years)')\n",
        "axes[1].set_xlabel('Segment')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Demographics analysis complete!\")"
      ],
      "metadata": {
        "id": "Kae141KRZOKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Add segment labels back to the main modeling dataset\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" ADDING SEGMENTS TO MAIN DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Merge segments back to modeling_data\n",
        "modeling_data = modeling_data.merge(\n",
        "    customer_level_data[['customer_id', 'segment', 'segment_name']],\n",
        "    on='customer_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(f\" Segments added to modeling_data!\")\n",
        "print(f\"   Shape: {modeling_data.shape}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n Sample with segments:\")\n",
        "display(modeling_data[['customer_id', 'segment', 'segment_name', 'offer_type_simple',\n",
        "                       'conv_14d_conversion', 'ret_30d_retention_active']].head(10))\n",
        "\n",
        "# Check distribution\n",
        "print(\"\\n Segment distribution in modeling_data:\")\n",
        "print(modeling_data['segment_name'].value_counts())\n",
        "\n",
        "print(\"\\n Segmentation complete and integrated!\")"
      ],
      "metadata": {
        "id": "-SrsUrunZUbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segment Profitability Analysis"
      ],
      "metadata": {
        "id": "Qbn4rjEmZbzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze segment profitability and offer response\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SEGMENT PROFITABILITY ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate profitability metrics by segment\n",
        "segment_profitability = modeling_data.groupby('segment_name').agg({\n",
        "    # Revenue metrics\n",
        "    'conv_14d_total_spent_in_window': ['mean', 'sum'],\n",
        "    'ret_30d_retention_total_spent': ['mean', 'sum'],\n",
        "\n",
        "    # Conversion metrics\n",
        "    'conv_14d_conversion': 'mean',\n",
        "    'conv_14d_offer_completed': 'mean',\n",
        "    'ret_30d_retention_active': 'mean',\n",
        "\n",
        "    # Engagement\n",
        "    'prev_view_rate': 'mean',\n",
        "    'prev_completion_rate': 'mean',\n",
        "\n",
        "    # Count\n",
        "    'customer_id': 'count'\n",
        "}).round(2)\n",
        "\n",
        "segment_profitability.columns = ['_'.join(col).strip() for col in segment_profitability.columns.values]\n",
        "segment_profitability = segment_profitability.rename(columns={\n",
        "    'conv_14d_total_spent_in_window_mean': 'avg_revenue_14d',\n",
        "    'conv_14d_total_spent_in_window_sum': 'total_revenue_14d',\n",
        "    'ret_30d_retention_total_spent_mean': 'avg_revenue_30d',\n",
        "    'ret_30d_retention_total_spent_sum': 'total_revenue_30d',\n",
        "    'conv_14d_conversion_mean': 'conversion_rate',\n",
        "    'conv_14d_offer_completed_mean': 'offer_completion_rate',\n",
        "    'ret_30d_retention_active_mean': 'retention_rate',\n",
        "    'prev_view_rate_mean': 'avg_view_rate',\n",
        "    'prev_completion_rate_mean': 'avg_completion_rate',\n",
        "    'customer_id_count': 'offer_instances'\n",
        "})\n",
        "\n",
        "print(\"\\n SEGMENT PROFITABILITY METRICS:\")\n",
        "print(\"=\" * 80)\n",
        "display(segment_profitability)\n",
        "\n",
        "# Calculate Customer Lifetime Value proxy\n",
        "segment_profitability['clv_proxy'] = (\n",
        "    segment_profitability['avg_revenue_30d'] *\n",
        "    segment_profitability['retention_rate'] * 12  # Annualized\n",
        ")\n",
        "\n",
        "print(\"\\n CUSTOMER LIFETIME VALUE (Proxy - Annualized):\")\n",
        "print(\"=\" * 80)\n",
        "display(segment_profitability[['avg_revenue_30d', 'retention_rate', 'clv_proxy']].sort_values('clv_proxy', ascending=False))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#F7DC6F']\n",
        "\n",
        "# 1. Average Revenue by Segment\n",
        "segment_profitability['avg_revenue_14d'].sort_values().plot(\n",
        "    kind='barh', ax=axes[0, 0], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[0, 0].set_title('Average Revenue per Offer (14d)', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Average Revenue ($)')\n",
        "axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 2. Conversion Rate by Segment\n",
        "segment_profitability['conversion_rate'].sort_values().plot(\n",
        "    kind='barh', ax=axes[0, 1], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[0, 1].set_title('Conversion Rate by Segment', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Conversion Rate')\n",
        "axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 3. Retention Rate by Segment\n",
        "segment_profitability['retention_rate'].sort_values().plot(\n",
        "    kind='barh', ax=axes[1, 0], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[1, 0].set_title('Retention Rate by Segment', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Retention Rate')\n",
        "axes[1, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 4. CLV Proxy by Segment\n",
        "segment_profitability['clv_proxy'].sort_values().plot(\n",
        "    kind='barh', ax=axes[1, 1], color=colors, edgecolor='black'\n",
        ")\n",
        "axes[1, 1].set_title('Customer Lifetime Value (Proxy)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('CLV Proxy ($)')\n",
        "axes[1, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Profitability analysis complete!\")"
      ],
      "metadata": {
        "id": "r8-RScTfZXUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segment Response to Different Offer Types"
      ],
      "metadata": {
        "id": "MQZYX9jNZiDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze how different segments respond to different offer types\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SEGMENT RESPONSE TO OFFER TYPES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Conversion rate by segment and offer type\n",
        "segment_offer_response = modeling_data.groupby(['segment_name', 'offer_type_simple']).agg({\n",
        "    'conv_14d_conversion': 'mean',\n",
        "    'conv_14d_offer_completed': 'mean',\n",
        "    'conv_14d_total_spent_in_window': 'mean',\n",
        "    'ret_30d_retention_active': 'mean',\n",
        "    'customer_id': 'count'\n",
        "}).round(3)\n",
        "\n",
        "segment_offer_response.columns = ['conversion_rate', 'completion_rate', 'avg_revenue', 'retention_rate', 'count']\n",
        "\n",
        "print(\"\\n CONVERSION RATE BY SEGMENT Ã— OFFER TYPE:\")\n",
        "print(\"=\" * 80)\n",
        "conversion_pivot = segment_offer_response['conversion_rate'].unstack()\n",
        "display(conversion_pivot)\n",
        "\n",
        "print(\"\\n AVERAGE REVENUE BY SEGMENT Ã— OFFER TYPE:\")\n",
        "print(\"=\" * 80)\n",
        "revenue_pivot = segment_offer_response['avg_revenue'].unstack()\n",
        "display(revenue_pivot)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Heatmap: Conversion Rate\n",
        "import seaborn as sns\n",
        "sns.heatmap(conversion_pivot, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "            ax=axes[0], cbar_kws={'label': 'Conversion Rate'})\n",
        "axes[0].set_title('Conversion Rate by Segment Ã— Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Offer Type')\n",
        "axes[0].set_ylabel('Segment')\n",
        "\n",
        "# Heatmap: Average Revenue\n",
        "sns.heatmap(revenue_pivot, annot=True, fmt='.2f', cmap='Greens',\n",
        "            ax=axes[1], cbar_kws={'label': 'Avg Revenue ($)'})\n",
        "axes[1].set_title('Average Revenue by Segment Ã— Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Offer Type')\n",
        "axes[1].set_ylabel('Segment')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n INSIGHTS:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Use this matrix to determine:\")\n",
        "print(\"â€¢ Which offer types work best for each segment\")\n",
        "print(\"â€¢ Which segments are most valuable for each offer type\")\n",
        "print(\"â€¢ Personalization strategies for the uplift model\")\n",
        "\n",
        "print(\"\\n Segment Ã— Offer analysis complete!\")"
      ],
      "metadata": {
        "id": "qjcE2ch7ZfM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Save all segmentation results\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SAVING SEGMENTATION RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save customer-level data with segments\n",
        "customer_level_data.to_csv('cleaned_data/customer_segments.csv', index=False)\n",
        "print(\" Saved: customer_segments.csv\")\n",
        "\n",
        "# Save segment profiles\n",
        "segment_profiles.to_csv('cleaned_data/segment_profiles.csv')\n",
        "print(\" Saved: segment_profiles.csv\")\n",
        "\n",
        "# Save segment profitability\n",
        "segment_profitability.to_csv('cleaned_data/segment_profitability.csv')\n",
        "print(\" Saved: segment_profitability.csv\")\n",
        "\n",
        "# Save modeling data with segments\n",
        "modeling_data.to_csv('cleaned_data/modeling_data_with_segments.csv', index=False)\n",
        "print(\" Saved: modeling_data_with_segments.csv\")\n",
        "\n",
        "modeling_data.to_pickle('cleaned_data/modeling_data_with_segments.pkl')\n",
        "print(\" Saved: modeling_data_with_segments.pkl\")\n",
        "\n",
        "# Save the scaler and kmeans model\n",
        "import pickle\n",
        "\n",
        "with open('cleaned_data/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\" Saved: scaler.pkl\")\n",
        "\n",
        "with open('cleaned_data/kmeans_model.pkl', 'wb') as f:\n",
        "    pickle.dump(kmeans_final, f)\n",
        "print(\"Saved: kmeans_model.pkl\")\n",
        "\n",
        "print(\"\\n All segmentation results saved!\")\n",
        "print(f\"   Location: cleaned_data/\")"
      ],
      "metadata": {
        "id": "s7HhICk3ZkhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Churn Target Variable"
      ],
      "metadata": {
        "id": "7vpsgK0gZxd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Define churn target variable\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" DEFINING CHURN TARGET VARIABLE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Churn = 1 if customer is NOT retained (ret_30d_retention_active = 0)\n",
        "modeling_data['churn_30d'] = (1 - modeling_data['ret_30d_retention_active']).astype(int)\n",
        "\n",
        "print(f\"\\n Churn Definition:\")\n",
        "print(\"   Churn = 1: Customer has NO transactions in next 30 days\")\n",
        "print(\"   Churn = 0: Customer has at least 1 transaction in next 30 days\")\n",
        "\n",
        "# Check distribution\n",
        "print(f\"\\n Churn Distribution:\")\n",
        "churn_dist = modeling_data['churn_30d'].value_counts()\n",
        "print(churn_dist)\n",
        "\n",
        "churn_rate = modeling_data['churn_30d'].mean() * 100\n",
        "print(f\"\\n Overall Churn Rate: {churn_rate:.2f}%\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Count plot\n",
        "churn_dist.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
        "axes[0].set_title('Churn Distribution (Count)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Churn Status')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Active (0)', 'Churned (1)'], rotation=0)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add count labels\n",
        "for i, v in enumerate(churn_dist):\n",
        "    axes[0].text(i, v + 1000, f'{v:,}\\n({v/len(modeling_data)*100:.1f}%)',\n",
        "                 ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Churn rate by segment\n",
        "churn_by_segment = modeling_data.groupby('segment_name')['churn_30d'].mean() * 100\n",
        "churn_by_segment.plot(kind='bar', ax=axes[1], color='#e74c3c', edgecolor='black')\n",
        "axes[1].set_title('Churn Rate by Segment', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Segment')\n",
        "axes[1].set_ylabel('Churn Rate (%)')\n",
        "axes[1].axhline(churn_rate, color='blue', linestyle='--', linewidth=2, label='Overall Avg')\n",
        "axes[1].legend()\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Churn target variable defined!\")"
      ],
      "metadata": {
        "id": "p1RZ2WJCZqOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select Features for Churn Prediction"
      ],
      "metadata": {
        "id": "7Z5zrh8VZ2Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Select features for churn prediction\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SELECTING FEATURES FOR CHURN PREDICTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select predictive features (everything BEFORE the outcome window)\n",
        "churn_features = [\n",
        "    # RFM features (most important)\n",
        "    'rfm_lifetime_recency_days',\n",
        "    'rfm_lifetime_frequency',\n",
        "    'rfm_lifetime_monetary_total',\n",
        "    'rfm_lifetime_monetary_mean',\n",
        "    'rfm_lifetime_monetary_max',\n",
        "    'rfm_lifetime_monetary_std',\n",
        "\n",
        "    # Recent behavior (30d, 90d)\n",
        "    'rfm_30d_recency_days',\n",
        "    'rfm_30d_frequency',\n",
        "    'rfm_30d_monetary_total',\n",
        "    'rfm_90d_recency_days',\n",
        "    'rfm_90d_frequency',\n",
        "    'rfm_90d_monetary_total',\n",
        "\n",
        "    # Offer engagement\n",
        "    'prev_offers_received',\n",
        "    'prev_offers_viewed',\n",
        "    'prev_offers_completed',\n",
        "    'prev_view_rate',\n",
        "    'prev_completion_rate',\n",
        "    'prev_completion_given_view_rate',\n",
        "\n",
        "    # Demographics\n",
        "    'age_clean',\n",
        "    'income_clean',\n",
        "    'tenure_at_offer',\n",
        "    'gender',\n",
        "\n",
        "    # Offer characteristics\n",
        "    'offer_type_simple',\n",
        "    'reward',\n",
        "    'min_spend_required',\n",
        "    'duration_days',\n",
        "\n",
        "    # Time features\n",
        "    'hour_of_day',\n",
        "    'day_of_week',\n",
        "    'is_weekend',\n",
        "\n",
        "    # Derived features\n",
        "    'customer_value_score',\n",
        "    'engagement_score',\n",
        "    'is_recently_active',\n",
        "    'is_high_spender',\n",
        "\n",
        "    # Segment\n",
        "    'segment'\n",
        "]\n",
        "\n",
        "print(f\"\\n Selected {len(churn_features)} features for churn prediction:\")\n",
        "for i, feat in enumerate(churn_features, 1):\n",
        "    print(f\"   {i:2d}. {feat}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\n Checking for missing values...\")\n",
        "missing = modeling_data[churn_features].isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "\n",
        "if len(missing) > 0:\n",
        "    print(f\"\\n  Features with missing values:\")\n",
        "    print(missing)\n",
        "    print(f\"\\n   Will handle these during preprocessing\")\n",
        "else:\n",
        "    print(\" No missing values in selected features!\")\n",
        "\n",
        "print(\"\\n Features selected!\")"
      ],
      "metadata": {
        "id": "QXLxhAQOZzwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Prepare data for churn prediction modeling\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" PREPARING DATA FOR MODELING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create a copy for modeling\n",
        "churn_model_data = modeling_data.copy()\n",
        "\n",
        "# First, let's see what features we actually have\n",
        "print(\"\\n Checking available features...\")\n",
        "available_features = []\n",
        "missing_features = []\n",
        "\n",
        "for feat in churn_features:\n",
        "    if feat in churn_model_data.columns:\n",
        "        available_features.append(feat)\n",
        "    else:\n",
        "        missing_features.append(feat)\n",
        "\n",
        "if len(missing_features) > 0:\n",
        "    print(f\"  Features not in dataframe: {missing_features}\")\n",
        "    print(f\"   Removing these from feature list...\")\n",
        "\n",
        "# Update churn_features to only include available features\n",
        "churn_features = available_features.copy()\n",
        "\n",
        "print(f\" Using {len(churn_features)} available features\")\n",
        "\n",
        "# Handle categorical variables\n",
        "print(\"\\n Encoding categorical variables...\")\n",
        "\n",
        "# One-hot encode categorical features\n",
        "categorical_features = ['gender', 'offer_type_simple']\n",
        "encoded_cols = []\n",
        "\n",
        "for cat_feat in categorical_features:\n",
        "    if cat_feat in churn_features:\n",
        "        print(f\"   Encoding {cat_feat}...\")\n",
        "        dummies = pd.get_dummies(churn_model_data[cat_feat], prefix=cat_feat, drop_first=True)\n",
        "        churn_model_data = pd.concat([churn_model_data, dummies], axis=1)\n",
        "        encoded_cols.extend(dummies.columns.tolist())\n",
        "\n",
        "        # Remove original categorical feature\n",
        "        churn_features.remove(cat_feat)\n",
        "\n",
        "# Add encoded columns to feature list\n",
        "churn_features.extend(encoded_cols)\n",
        "\n",
        "print(f\" Categorical variables encoded\")\n",
        "print(f\"   Total features now: {len(churn_features)}\")\n",
        "\n",
        "# Verify all features exist\n",
        "print(\"\\n Final feature verification...\")\n",
        "final_features = []\n",
        "for feat in churn_features:\n",
        "    if feat in churn_model_data.columns:\n",
        "        final_features.append(feat)\n",
        "    else:\n",
        "        print(f\"     Skipping missing feature: {feat}\")\n",
        "\n",
        "churn_features = final_features\n",
        "print(f\" Final feature count: {len(churn_features)}\")\n",
        "\n",
        "# Handle missing values\n",
        "print(\"\\nðŸ”§ Handling missing values...\")\n",
        "print(f\"\\nMissing values by feature:\")\n",
        "missing_counts = churn_model_data[churn_features].isnull().sum()\n",
        "missing_counts = missing_counts[missing_counts > 0]\n",
        "\n",
        "if len(missing_counts) > 0:\n",
        "    print(missing_counts)\n",
        "\n",
        "    # Fill missing values\n",
        "    for feat in churn_features:\n",
        "        if churn_model_data[feat].isnull().any():\n",
        "            # Check data type\n",
        "            if churn_model_data[feat].dtype in ['float64', 'int64']:\n",
        "                # Fill with median for numeric\n",
        "                fill_value = churn_model_data[feat].median()\n",
        "                if pd.isna(fill_value):\n",
        "                    fill_value = 0\n",
        "                churn_model_data[feat].fillna(fill_value, inplace=True)\n",
        "            else:\n",
        "                # Fill with mode for categorical\n",
        "                mode_val = churn_model_data[feat].mode()\n",
        "                fill_value = mode_val[0] if len(mode_val) > 0 else 0\n",
        "                churn_model_data[feat].fillna(fill_value, inplace=True)\n",
        "\n",
        "            print(f\"   Filled {feat}\")\n",
        "else:\n",
        "    print(\"No missing values!\")\n",
        "\n",
        "print(f\"\\n Missing values handled\")\n",
        "\n",
        "# Prepare X and y\n",
        "X = churn_model_data[churn_features].copy()\n",
        "y = churn_model_data['churn_30d'].copy()\n",
        "\n",
        "# Final safety check - replace any remaining NaN with 0\n",
        "X = X.fillna(0)\n",
        "\n",
        "print(f\"\\n Final verification:\")\n",
        "print(f\"   X has NaN: {X.isnull().any().any()}\")\n",
        "print(f\"   y has NaN: {y.isnull().any()}\")\n",
        "\n",
        "print(f\"\\n Modeling Dataset:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n",
        "print(f\"   Features: {X.shape[1]}\")\n",
        "print(f\"   Samples: {X.shape[0]:,}\")\n",
        "\n",
        "# Check class balance\n",
        "print(f\"\\n Class Distribution:\")\n",
        "print(y.value_counts())\n",
        "print(f\"\\n   Churn rate: {y.mean()*100:.2f}%\")\n",
        "\n",
        "print(\"\\n Data prepared for modeling!\")"
      ],
      "metadata": {
        "id": "y9dsjxeWZ4c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Split data into train and test sets\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"  TRAIN-TEST SPLIT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y  # Maintain class balance\n",
        ")\n",
        "\n",
        "print(f\"\\n Data Split:\")\n",
        "print(f\"   Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n Class Distribution in Train Set:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"   Churn rate: {y_train.mean()*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n Class Distribution in Test Set:\")\n",
        "print(y_test.value_counts())\n",
        "print(f\"   Churn rate: {y_test.mean()*100:.2f}%\")\n",
        "\n",
        "# Scale features\n",
        "print(\"\\n  Scaling features...\")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_churn = StandardScaler()\n",
        "X_train_scaled = scaler_churn.fit_transform(X_train)\n",
        "X_test_scaled = scaler_churn.transform(X_test)\n",
        "\n",
        "print(f\" Features scaled using StandardScaler\")\n",
        "\n",
        "# Convert back to DataFrames for easier handling\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"\\n Train-test split complete!\")"
      ],
      "metadata": {
        "id": "hWhS4ETbZ7w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train Logistic Regression model (baseline)\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" TRAINING LOGISTIC REGRESSION (BASELINE MODEL)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score\n",
        ")\n",
        "\n",
        "print(\"\\n Training Logistic Regression...\")\n",
        "\n",
        "# Train model\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',  # Handle class imbalance\n",
        "    solver='lbfgs'\n",
        ")\n",
        "\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\" Model trained!\")\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
        "y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
        "\n",
        "y_train_proba_lr = lr_model.predict_proba(X_train_scaled)[:, 1]\n",
        "y_test_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n LOGISTIC REGRESSION PERFORMANCE:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n Training Set:\")\n",
        "print(classification_report(y_train, y_train_pred_lr, target_names=['Active', 'Churned']))\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_train, y_train_proba_lr):.4f}\")\n",
        "\n",
        "print(\"\\n Test Set:\")\n",
        "print(classification_report(y_test, y_test_pred_lr, target_names=['Active', 'Churned']))\n",
        "test_auc_lr = roc_auc_score(y_test, y_test_proba_lr)\n",
        "print(f\"ROC-AUC Score: {test_auc_lr:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\n Confusion Matrix (Test Set):\")\n",
        "cm_lr = confusion_matrix(y_test, y_test_pred_lr)\n",
        "print(cm_lr)\n",
        "print(f\"\\n   True Negatives (TN): {cm_lr[0,0]:,}\")\n",
        "print(f\"   False Positives (FP): {cm_lr[0,1]:,}\")\n",
        "print(f\"   False Negatives (FN): {cm_lr[1,0]:,}\")\n",
        "print(f\"   True Positives (TP): {cm_lr[1,1]:,}\")\n",
        "\n",
        "print(\"\\n Logistic Regression evaluation complete!\")"
      ],
      "metadata": {
        "id": "H9R3c1ZBZ_2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train XGBoost model (high performance)\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" TRAINING XGBOOST MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"\\n Training XGBoost classifier...\")\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),  # Handle imbalance\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\" XGBoost model trained!\")\n",
        "\n",
        "# Predictions\n",
        "y_train_pred_xgb = xgb_model.predict(X_train_scaled)\n",
        "y_test_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "y_train_proba_xgb = xgb_model.predict_proba(X_train_scaled)[:, 1]\n",
        "y_test_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n XGBOOST PERFORMANCE:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n Training Set:\")\n",
        "print(classification_report(y_train, y_train_pred_xgb, target_names=['Active', 'Churned']))\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_train, y_train_proba_xgb):.4f}\")\n",
        "\n",
        "print(\"\\n Test Set:\")\n",
        "print(classification_report(y_test, y_test_pred_xgb, target_names=['Active', 'Churned']))\n",
        "test_auc_xgb = roc_auc_score(y_test, y_test_proba_xgb)\n",
        "print(f\"ROC-AUC Score: {test_auc_xgb:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\n Confusion Matrix (Test Set):\")\n",
        "cm_xgb = confusion_matrix(y_test, y_test_pred_xgb)\n",
        "print(cm_xgb)\n",
        "print(f\"\\n   True Negatives (TN): {cm_xgb[0,0]:,}\")\n",
        "print(f\"   False Positives (FP): {cm_xgb[0,1]:,}\")\n",
        "print(f\"   False Negatives (FN): {cm_xgb[1,0]:,}\")\n",
        "print(f\"   True Positives (TP): {cm_xgb[1,1]:,}\")\n",
        "\n",
        "print(\"\\n XGBoost evaluation complete!\")"
      ],
      "metadata": {
        "id": "ZYek-G-UaDl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compare Logistic Regression vs XGBoost\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" MODEL COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create comparison dataframe\n",
        "model_comparison = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'XGBoost'],\n",
        "    'Test ROC-AUC': [test_auc_lr, test_auc_xgb],\n",
        "})\n",
        "\n",
        "print(\"\\n Model Performance Comparison:\")\n",
        "display(model_comparison)\n",
        "\n",
        "# Visualize ROC curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# ROC Curve\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_test_proba_lr)\n",
        "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_test_proba_xgb)\n",
        "\n",
        "axes[0].plot(fpr_lr, tpr_lr, linewidth=2, label=f'Logistic Regression (AUC={test_auc_lr:.3f})', color='#3498db')\n",
        "axes[0].plot(fpr_xgb, tpr_xgb, linewidth=2, label=f'XGBoost (AUC={test_auc_xgb:.3f})', color='#e74c3c')\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
        "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
        "axes[0].set_title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision_lr, recall_lr, _ = precision_recall_curve(y_test, y_test_proba_lr)\n",
        "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, y_test_proba_xgb)\n",
        "\n",
        "ap_lr = average_precision_score(y_test, y_test_proba_lr)\n",
        "ap_xgb = average_precision_score(y_test, y_test_proba_xgb)\n",
        "\n",
        "axes[1].plot(recall_lr, precision_lr, linewidth=2, label=f'Logistic Regression (AP={ap_lr:.3f})', color='#3498db')\n",
        "axes[1].plot(recall_xgb, precision_xgb, linewidth=2, label=f'XGBoost (AP={ap_xgb:.3f})', color='#e74c3c')\n",
        "axes[1].set_xlabel('Recall', fontsize=12)\n",
        "axes[1].set_ylabel('Precision', fontsize=12)\n",
        "axes[1].set_title('Precision-Recall Curve Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Determine best model\n",
        "best_model_name = 'XGBoost' if test_auc_xgb > test_auc_lr else 'Logistic Regression'\n",
        "best_model = xgb_model if test_auc_xgb > test_auc_lr else lr_model\n",
        "best_proba = y_test_proba_xgb if test_auc_xgb > test_auc_lr else y_test_proba_lr\n",
        "\n",
        "print(f\"\\n BEST MODEL: {best_model_name}\")\n",
        "print(f\"   Test ROC-AUC: {max(test_auc_lr, test_auc_xgb):.4f}\")\n",
        "\n",
        "print(\"\\n Model comparison complete!\")"
      ],
      "metadata": {
        "id": "3FDryf9WayzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze feature importance from XGBoost\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\n Top 20 Most Important Features:\")\n",
        "print(\"=\" * 80)\n",
        "display(feature_importance.head(20))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "top_features = feature_importance.head(20)\n",
        "ax.barh(range(len(top_features)), top_features['importance'], color='#3498db', edgecolor='black')\n",
        "ax.set_yticks(range(len(top_features)))\n",
        "ax.set_yticklabels(top_features['feature'])\n",
        "ax.set_xlabel('Importance', fontsize=12)\n",
        "ax.set_title('Top 20 Feature Importances (XGBoost)', fontsize=14, fontweight='bold')\n",
        "ax.invert_yaxis()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Feature importance analysis complete!\")"
      ],
      "metadata": {
        "id": "u2f_9e97a4D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Generate SHAP explanations for model interpretability\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" GENERATING SHAP EXPLANATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import shap\n",
        "\n",
        "print(\"\\n Computing SHAP values (this may take a few minutes)...\")\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.TreeExplainer(xgb_model)\n",
        "\n",
        "# Calculate SHAP values for test set (use a sample if too large)\n",
        "sample_size = min(1000, len(X_test_scaled))\n",
        "X_test_sample = X_test_scaled.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(f\"   Computing SHAP values for {sample_size} test samples...\")\n",
        "shap_values = explainer.shap_values(X_test_sample)\n",
        "\n",
        "print(\" SHAP values computed!\")\n",
        "\n",
        "# Summary plot\n",
        "print(\"\\n Generating SHAP summary plot...\")\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(shap_values, X_test_sample, plot_type=\"bar\", show=False)\n",
        "plt.title('SHAP Feature Importance', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed summary plot\n",
        "print(\"\\n Generating detailed SHAP summary plot...\")\n",
        "plt.figure(figsize=(12, 10))\n",
        "shap.summary_plot(shap_values, X_test_sample, show=False)\n",
        "plt.title('SHAP Summary Plot (Feature Impact)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n SHAP explanations generated!\")\n",
        "\n",
        "print(\"\\n INTERPRETATION:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "The SHAP plots show:\n",
        "- Which features are most important for predictions\n",
        "- How feature values impact predictions (red = high value, blue = low value)\n",
        "- Positive SHAP value = increases churn probability\n",
        "- Negative SHAP value = decreases churn probability\n",
        "\n",
        "Top features typically include:\n",
        "- Recency (days since last purchase)\n",
        "- Frequency (transaction count)\n",
        "- Monetary (spending amount)\n",
        "- Offer engagement rates\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Vn6A68vFa64z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Generate churn risk scores for all customers in the dataset\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" GENERATING CHURN RISK SCORES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n Generating predictions for all data...\")\n",
        "\n",
        "# Scale all data\n",
        "X_all_scaled = scaler_churn.transform(X)\n",
        "\n",
        "# Predict churn probabilities\n",
        "churn_probabilities = xgb_model.predict_proba(X_all_scaled)[:, 1]\n",
        "\n",
        "# Add to modeling data\n",
        "modeling_data['churn_risk_score'] = churn_probabilities\n",
        "\n",
        "print(\" Churn risk scores generated!\")\n",
        "\n",
        "# Analyze risk distribution\n",
        "print(\"\\n Churn Risk Distribution:\")\n",
        "print(modeling_data['churn_risk_score'].describe())\n",
        "\n",
        "# Risk categories\n",
        "modeling_data['churn_risk_category'] = pd.cut(\n",
        "    modeling_data['churn_risk_score'],\n",
        "    bins=[0, 0.3, 0.6, 1.0],\n",
        "    labels=['Low Risk', 'Medium Risk', 'High Risk']\n",
        ")\n",
        "\n",
        "print(\"\\n Risk Category Distribution:\")\n",
        "print(modeling_data['churn_risk_category'].value_counts())\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(modeling_data['churn_risk_score'], bins=50, color='#e74c3c', edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Churn Risk Score', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].set_title('Distribution of Churn Risk Scores', fontsize=14, fontweight='bold')\n",
        "axes[0].axvline(modeling_data['churn_risk_score'].median(), color='blue',\n",
        "                linestyle='--', linewidth=2, label=f'Median: {modeling_data[\"churn_risk_score\"].median():.3f}')\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Risk categories\n",
        "risk_counts = modeling_data['churn_risk_category'].value_counts()\n",
        "axes[1].bar(range(len(risk_counts)), risk_counts.values,\n",
        "            color=['#2ecc71', '#f39c12', '#e74c3c'], edgecolor='black')\n",
        "axes[1].set_xticks(range(len(risk_counts)))\n",
        "axes[1].set_xticklabels(risk_counts.index, rotation=0)\n",
        "axes[1].set_ylabel('Count', fontsize=12)\n",
        "axes[1].set_title('Churn Risk Categories', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add count labels\n",
        "for i, v in enumerate(risk_counts.values):\n",
        "    axes[1].text(i, v + 500, f'{v:,}\\n({v/len(modeling_data)*100:.1f}%)',\n",
        "                 ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Churn risk scores added to dataset!\")"
      ],
      "metadata": {
        "id": "SqfDe3ZAa92A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze churn risk by customer segment\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CHURN RISK BY SEGMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Average churn risk by segment\n",
        "segment_risk = modeling_data.groupby('segment_name').agg({\n",
        "    'churn_risk_score': ['mean', 'median', 'std'],\n",
        "    'churn_30d': 'mean',\n",
        "    'customer_id': 'count'\n",
        "}).round(3)\n",
        "\n",
        "segment_risk.columns = ['avg_risk', 'median_risk', 'std_risk', 'actual_churn_rate', 'count']\n",
        "\n",
        "print(\"\\n Churn Risk by Segment:\")\n",
        "display(segment_risk.sort_values('avg_risk', ascending=False))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Average risk by segment\n",
        "segment_risk['avg_risk'].sort_values().plot(\n",
        "    kind='barh', ax=axes[0], color='#e74c3c', edgecolor='black'\n",
        ")\n",
        "axes[0].set_xlabel('Average Churn Risk Score', fontsize=12)\n",
        "axes[0].set_title('Average Churn Risk by Segment', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Compare predicted vs actual\n",
        "comparison = segment_risk[['avg_risk', 'actual_churn_rate']].sort_values('avg_risk')\n",
        "comparison.plot(kind='barh', ax=axes[1], color=['#e74c3c', '#3498db'], edgecolor='black')\n",
        "axes[1].set_xlabel('Rate', fontsize=12)\n",
        "axes[1].set_title('Predicted Risk vs Actual Churn Rate', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(['Predicted Risk', 'Actual Churn Rate'])\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Segment analysis complete!\")"
      ],
      "metadata": {
        "id": "0MLqf8xpbIbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Save churn prediction models and results\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SAVING CHURN MODELS AND RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save models\n",
        "with open('cleaned_data/logistic_regression_churn.pkl', 'wb') as f:\n",
        "    pickle.dump(lr_model, f)\n",
        "print(\" Saved: logistic_regression_churn.pkl\")\n",
        "\n",
        "with open('cleaned_data/xgboost_churn.pkl', 'wb') as f:\n",
        "    pickle.dump(xgb_model, f)\n",
        "print(\" Saved: xgboost_churn.pkl\")\n",
        "\n",
        "# Save scaler\n",
        "with open('cleaned_data/scaler_churn.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_churn, f)\n",
        "print(\" Saved: scaler_churn.pkl\")\n",
        "\n",
        "# Save feature list\n",
        "with open('cleaned_data/churn_features.pkl', 'wb') as f:\n",
        "    pickle.dump(churn_features, f)\n",
        "print(\" Saved: churn_features.pkl\")\n",
        "\n",
        "# Save modeling data with churn scores\n",
        "modeling_data.to_csv('cleaned_data/modeling_data_with_churn_scores.csv', index=False)\n",
        "print(\" Saved: modeling_data_with_churn_scores.csv\")\n",
        "\n",
        "modeling_data.to_pickle('cleaned_data/modeling_data_with_churn_scores.pkl')\n",
        "print(\" Saved: modeling_data_with_churn_scores.pkl\")\n",
        "\n",
        "# Save model performance metrics\n",
        "model_metrics = {\n",
        "    'logistic_regression': {\n",
        "        'test_auc': test_auc_lr,\n",
        "        'test_predictions': y_test_pred_lr,\n",
        "        'test_probabilities': y_test_proba_lr\n",
        "    },\n",
        "    'xgboost': {\n",
        "        'test_auc': test_auc_xgb,\n",
        "        'test_predictions': y_test_pred_xgb,\n",
        "        'test_probabilities': y_test_proba_xgb\n",
        "    },\n",
        "    'best_model': best_model_name,\n",
        "    'feature_importance': feature_importance\n",
        "}\n",
        "\n",
        "with open('cleaned_data/churn_model_metrics.pkl', 'wb') as f:\n",
        "    pickle.dump(model_metrics, f)\n",
        "print(\" Saved: churn_model_metrics.pkl\")\n",
        "\n",
        "print(\"\\n All churn models and results saved!\")\n",
        "print(f\"   Location: cleaned_data/\")"
      ],
      "metadata": {
        "id": "nAnszlXdbLc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create a summary report of churn risk analysis\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CHURN RISK SUMMARY REPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "CHURN PREDICTION MODEL SUMMARY\n",
        "================================\n",
        "\n",
        " DATASET:\n",
        "-----------\n",
        "Total Samples: {len(X):,}\n",
        "Training Samples: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\n",
        "Test Samples: {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\n",
        "Features: {len(churn_features)}\n",
        "Target: Churn in next 30 days\n",
        "\n",
        " CLASS DISTRIBUTION:\n",
        "----------------------\n",
        "Overall Churn Rate: {modeling_data['churn_30d'].mean()*100:.2f}%\n",
        "Active Customers: {(modeling_data['churn_30d']==0).sum():,} ({(1-modeling_data['churn_30d'].mean())*100:.1f}%)\n",
        "Churned Customers: {(modeling_data['churn_30d']==1).sum():,} ({modeling_data['churn_30d'].mean()*100:.1f}%)\n",
        "\n",
        " MODEL PERFORMANCE:\n",
        "--------------------\n",
        "Logistic Regression:\n",
        "  - Test ROC-AUC: {test_auc_lr:.4f}\n",
        "\n",
        "XGBoost:\n",
        "  - Test ROC-AUC: {test_auc_xgb:.4f}\n",
        "\n",
        "Best Model: {best_model_name}\n",
        "Best Test ROC-AUC: {max(test_auc_lr, test_auc_xgb):.4f}\n",
        "\n",
        " TOP 5 PREDICTIVE FEATURES:\n",
        "------------------------------\n",
        "\"\"\")\n",
        "\n",
        "for i, row in feature_importance.head(5).iterrows():\n",
        "    print(f\"   {i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(f\"\"\"\n",
        "CHURN RISK DISTRIBUTION:\n",
        "---------------------------\n",
        "Mean Risk Score: {modeling_data['churn_risk_score'].mean():.3f}\n",
        "Median Risk Score: {modeling_data['churn_risk_score'].median():.3f}\n",
        "Std Risk Score: {modeling_data['churn_risk_score'].std():.3f}\n",
        "\n",
        "Risk Categories:\n",
        "\"\"\")\n",
        "\n",
        "for cat, count in modeling_data['churn_risk_category'].value_counts().items():\n",
        "    pct = count / len(modeling_data) * 100\n",
        "    print(f\"  â€¢ {cat}: {count:,} customers ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\"\"\n",
        " SEGMENT RISK ANALYSIS:\n",
        "-------------------------\n",
        "Highest Risk Segment: {segment_risk.sort_values('avg_risk', ascending=False).index[0]}\n",
        "  - Avg Risk: {segment_risk.sort_values('avg_risk', ascending=False)['avg_risk'].iloc[0]:.3f}\n",
        "  - Actual Churn: {segment_risk.sort_values('avg_risk', ascending=False)['actual_churn_rate'].iloc[0]:.3f}\n",
        "\n",
        "Lowest Risk Segment: {segment_risk.sort_values('avg_risk', ascending=True).index[0]}\n",
        "  - Avg Risk: {segment_risk.sort_values('avg_risk', ascending=True)['avg_risk'].iloc[0]:.3f}\n",
        "  - Actual Churn: {segment_risk.sort_values('avg_risk', ascending=True)['actual_churn_rate'].iloc[0]:.3f}\n",
        "\n",
        "KEY INSIGHTS:\n",
        "----------------\n",
        "- Churn prediction model successfully identifies at-risk customers\n",
        "- Model can prioritize retention efforts on high-risk customers\n",
        "- Segments show different churn risk profiles\n",
        "- Top features align with business intuition (RFM metrics)\n",
        "- Predictions are calibrated and ready for use in NBA system\n",
        "\n",
        "READY FOR STEP 5: UPLIFT MODELING\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "u03dCndXbSrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Prepare data for uplift modeling\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" PREPARING DATA FOR UPLIFT MODELING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# We'll use the modeling_data which has all features and outcomes\n",
        "uplift_data = modeling_data.copy()\n",
        "\n",
        "print(f\"\\n Uplift Dataset:\")\n",
        "print(f\"   Total samples: {len(uplift_data):,}\")\n",
        "print(f\"   Unique customers: {uplift_data['customer_id'].nunique():,}\")\n",
        "\n",
        "# Treatment variable\n",
        "print(f\"\\n Treatment Distribution:\")\n",
        "treatment_dist = uplift_data['offer_type_simple'].value_counts()\n",
        "print(treatment_dist)\n",
        "print(f\"\\nPercentages:\")\n",
        "print((treatment_dist / len(uplift_data) * 100).round(2))\n",
        "\n",
        "# Outcome variables\n",
        "print(f\"\\n Outcome Variables:\")\n",
        "print(f\"   Conversion rate: {uplift_data['conv_14d_conversion'].mean()*100:.2f}%\")\n",
        "print(f\"   Average revenue (14d): ${uplift_data['conv_14d_total_spent_in_window'].mean():.2f}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Treatment distribution\n",
        "treatment_dist.plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black')\n",
        "axes[0].set_title('Treatment Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Offer Type')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].tick_params(axis='x', rotation=0)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Conversion by treatment\n",
        "conv_by_treatment = uplift_data.groupby('offer_type_simple')['conv_14d_conversion'].mean()\n",
        "conv_by_treatment.plot(kind='bar', ax=axes[1], color=['#FF6B6B', '#4ECDC4', '#45B7D1'], edgecolor='black')\n",
        "axes[1].set_title('Conversion Rate by Offer Type', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Offer Type')\n",
        "axes[1].set_ylabel('Conversion Rate')\n",
        "axes[1].tick_params(axis='x', rotation=0)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "axes[1].axhline(uplift_data['conv_14d_conversion'].mean(), color='red', linestyle='--', label='Overall Avg')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Data prepared for uplift modeling!\")"
      ],
      "metadata": {
        "id": "FZZdMk4FbXIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Select features for uplift modeling (same as churn features but without treatment)\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SELECTING FEATURES FOR UPLIFT MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use same features as churn model, but exclude treatment-related features\n",
        "uplift_features = [feat for feat in churn_features\n",
        "                   if 'offer_type' not in feat.lower() and\n",
        "                   'reward' not in feat.lower() and\n",
        "                   'min_spend' not in feat.lower() and\n",
        "                   'duration' not in feat.lower()]\n",
        "\n",
        "print(f\"\\n Selected {len(uplift_features)} features for uplift modeling:\")\n",
        "print(f\"\\nFeature categories:\")\n",
        "print(f\"   â€¢ RFM features\")\n",
        "print(f\"   â€¢ Engagement features\")\n",
        "print(f\"   â€¢ Demographics\")\n",
        "print(f\"   â€¢ Segment membership\")\n",
        "print(f\"   â€¢ Time-based features\")\n",
        "\n",
        "# Verify features exist\n",
        "missing_uplift = [f for f in uplift_features if f not in uplift_data.columns]\n",
        "if missing_uplift:\n",
        "    print(f\"\\n  Removing missing features: {missing_uplift}\")\n",
        "    uplift_features = [f for f in uplift_features if f in uplift_data.columns]\n",
        "\n",
        "print(f\"\\n Final feature count: {len(uplift_features)}\")"
      ],
      "metadata": {
        "id": "EMkYZcI0b5Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Build T-Learner models for each treatment\n",
        "We'll compare each treatment (BOGO, Discount, Informational) vs baseline\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸŽ“ BUILDING T-LEARNER UPLIFT MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# We'll build models comparing each treatment to \"Informational\" as baseline\n",
        "# (Informational has lowest cost and can serve as control)\n",
        "\n",
        "print(\"\\n T-Learner Approach:\")\n",
        "print(\"   Train separate models for each treatment group\")\n",
        "print(\"   Uplift = P(Y|Treatment) - P(Y|Control)\")\n",
        "\n",
        "# Define treatments to model\n",
        "treatments = ['BOGO', 'Discount', 'Informational']\n",
        "control_group = 'Informational'  # Lowest cost offer\n",
        "\n",
        "print(f\"\\n Treatments to evaluate: {treatments}\")\n",
        "print(f\"   Control/Baseline: {control_group}\")\n",
        "\n",
        "# Store models\n",
        "treatment_models = {}\n",
        "control_models = {}\n",
        "\n",
        "# Prepare data - fill missing values\n",
        "uplift_data_clean = uplift_data.copy()\n",
        "for feat in uplift_features:\n",
        "    if uplift_data_clean[feat].isnull().any():\n",
        "        uplift_data_clean[feat].fillna(uplift_data_clean[feat].median(), inplace=True)\n",
        "\n",
        "# Scale features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_uplift = StandardScaler()\n",
        "X_uplift = uplift_data_clean[uplift_features]\n",
        "X_uplift_scaled = scaler_uplift.fit_transform(X_uplift)\n",
        "X_uplift_scaled = pd.DataFrame(X_uplift_scaled, columns=uplift_features, index=X_uplift.index)\n",
        "\n",
        "y_conv = uplift_data_clean['conv_14d_conversion']\n",
        "y_revenue = uplift_data_clean['conv_14d_total_spent_in_window']\n",
        "\n",
        "print(\"\\n Training models for each treatment...\")\n",
        "\n",
        "for treatment in treatments:\n",
        "    print(f\"\\nðŸ”¹ Training models for: {treatment}\")\n",
        "\n",
        "    # Get treatment group data\n",
        "    treatment_mask = uplift_data_clean['offer_type_simple'] == treatment\n",
        "    X_treatment = X_uplift_scaled[treatment_mask]\n",
        "    y_treatment = y_conv[treatment_mask]\n",
        "\n",
        "    print(f\"   Treatment group size: {len(X_treatment):,}\")\n",
        "    print(f\"   Conversion rate: {y_treatment.mean()*100:.2f}%\")\n",
        "\n",
        "    # Train model for treatment group\n",
        "    model_treatment = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model_treatment.fit(X_treatment, y_treatment)\n",
        "    treatment_models[treatment] = model_treatment\n",
        "\n",
        "    print(f\"   Model trained for {treatment}\")\n",
        "\n",
        "# Train control model\n",
        "print(f\"\\n Training control model: {control_group}\")\n",
        "control_mask = uplift_data_clean['offer_type_simple'] == control_group\n",
        "X_control = X_uplift_scaled[control_mask]\n",
        "y_control = y_conv[control_mask]\n",
        "\n",
        "print(f\"   Control group size: {len(X_control):,}\")\n",
        "print(f\"   Conversion rate: {y_control.mean()*100:.2f}%\")\n",
        "\n",
        "model_control = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model_control.fit(X_control, y_control)\n",
        "\n",
        "print(f\"   Control model trained\")\n",
        "\n",
        "print(\"\\n All T-Learner models trained!\")"
      ],
      "metadata": {
        "id": "c6LQ9quQb76o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate uplift scores for each customer-treatment combination\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CALCULATING UPLIFT SCORES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n Generating uplift predictions for all customers...\")\n",
        "\n",
        "# Get predictions for all data\n",
        "control_proba = model_control.predict_proba(X_uplift_scaled)[:, 1]\n",
        "\n",
        "# Calculate uplift for each treatment\n",
        "uplift_scores = {}\n",
        "\n",
        "for treatment in treatments:\n",
        "    if treatment == control_group:\n",
        "        # Control vs itself = 0 uplift\n",
        "        uplift_scores[treatment] = np.zeros(len(X_uplift_scaled))\n",
        "    else:\n",
        "        # Treatment probability - Control probability\n",
        "        treatment_proba = treatment_models[treatment].predict_proba(X_uplift_scaled)[:, 1]\n",
        "        uplift = treatment_proba - control_proba\n",
        "        uplift_scores[treatment] = uplift\n",
        "\n",
        "    # Add to dataframe\n",
        "    uplift_data_clean[f'uplift_{treatment}'] = uplift_scores[treatment]\n",
        "\n",
        "    print(f\" Calculated uplift for {treatment}\")\n",
        "    print(f\"   Mean uplift: {uplift_scores[treatment].mean():.4f}\")\n",
        "    print(f\"   Median uplift: {np.median(uplift_scores[treatment]):.4f}\")\n",
        "\n",
        "print(\"\\n All uplift scores calculated!\")\n",
        "\n",
        "# Visualize uplift distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "for i, treatment in enumerate(['BOGO', 'Discount']):  # Skip Informational (all zeros)\n",
        "    if treatment != control_group:\n",
        "        axes[i].hist(uplift_scores[treatment], bins=50, color='#3498db', edgecolor='black', alpha=0.7)\n",
        "        axes[i].set_xlabel('Uplift Score', fontsize=12)\n",
        "        axes[i].set_ylabel('Frequency', fontsize=12)\n",
        "        axes[i].set_title(f'Uplift Distribution: {treatment} vs {control_group}', fontsize=12, fontweight='bold')\n",
        "        axes[i].axvline(0, color='red', linestyle='--', linewidth=2, label='No Effect')\n",
        "        axes[i].axvline(uplift_scores[treatment].mean(), color='green', linestyle='--',\n",
        "                       linewidth=2, label=f'Mean: {uplift_scores[treatment].mean():.4f}')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Comparison plot\n",
        "uplift_comparison = pd.DataFrame({\n",
        "    'BOGO': uplift_scores['BOGO'],\n",
        "    'Discount': uplift_scores['Discount']\n",
        "})\n",
        "uplift_comparison.boxplot(ax=axes[2])\n",
        "axes[2].set_ylabel('Uplift Score', fontsize=12)\n",
        "axes[2].set_title('Uplift Comparison: BOGO vs Discount', fontsize=12, fontweight='bold')\n",
        "axes[2].axhline(0, color='red', linestyle='--', linewidth=2, label='No Effect')\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Uplift visualization complete!\")"
      ],
      "metadata": {
        "id": "2wE0g6tEb_C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate expected profit considering offer costs\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CALCULATING EXPECTED PROFIT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define offer costs (from Step 1)\n",
        "OFFER_COSTS = {\n",
        "    'BOGO': 5.0,\n",
        "    'Discount': 3.0,\n",
        "    'Informational': 0.5,\n",
        "    'No Offer': 0.0\n",
        "}\n",
        "\n",
        "print(f\"\\n Offer Costs:\")\n",
        "for offer, cost in OFFER_COSTS.items():\n",
        "    print(f\"   {offer:20s}: ${cost:.2f}\")\n",
        "\n",
        "# Average revenue per conversion\n",
        "avg_revenue_per_conversion = uplift_data_clean[\n",
        "    uplift_data_clean['conv_14d_conversion'] == 1\n",
        "]['conv_14d_total_spent_in_window'].mean()\n",
        "\n",
        "print(f\"\\n Average revenue per conversion: ${avg_revenue_per_conversion:.2f}\")\n",
        "\n",
        "# Calculate expected profit for each treatment\n",
        "for treatment in treatments:\n",
        "    # Expected incremental conversions\n",
        "    expected_conversions = uplift_scores[treatment]\n",
        "\n",
        "    # Expected incremental revenue\n",
        "    expected_revenue = expected_conversions * avg_revenue_per_conversion\n",
        "\n",
        "    # Expected profit = Revenue - Cost\n",
        "    offer_cost = OFFER_COSTS[treatment]\n",
        "    expected_profit = expected_revenue - offer_cost\n",
        "\n",
        "    # Add to dataframe\n",
        "    uplift_data_clean[f'expected_revenue_{treatment}'] = expected_revenue\n",
        "    uplift_data_clean[f'expected_profit_{treatment}'] = expected_profit\n",
        "\n",
        "    print(f\"\\n {treatment}:\")\n",
        "    print(f\"   Avg expected revenue: ${expected_revenue.mean():.2f}\")\n",
        "    print(f\"   Offer cost: ${offer_cost:.2f}\")\n",
        "    print(f\"   Avg expected profit: ${expected_profit.mean():.2f}\")\n",
        "    print(f\"   Customers with positive profit: {(expected_profit > 0).sum():,} ({(expected_profit > 0).mean()*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n Expected profit calculated!\")"
      ],
      "metadata": {
        "id": "76JOBRFbcEuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Generate personalized Next Best Action recommendations\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" GENERATING NEXT BEST ACTION RECOMMENDATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n Determining best action for each customer...\")\n",
        "\n",
        "# For each customer, find the action with highest expected profit\n",
        "profit_columns = [f'expected_profit_{treatment}' for treatment in treatments]\n",
        "\n",
        "# Find best action\n",
        "uplift_data_clean['best_action'] = uplift_data_clean[profit_columns].idxmax(axis=1)\n",
        "uplift_data_clean['best_action'] = uplift_data_clean['best_action'].str.replace('expected_profit_', '')\n",
        "\n",
        "# Get the expected profit of best action\n",
        "uplift_data_clean['best_action_profit'] = uplift_data_clean[profit_columns].max(axis=1)\n",
        "\n",
        "# Get the uplift of best action\n",
        "uplift_data_clean['best_action_uplift'] = uplift_data_clean.apply(\n",
        "    lambda row: row[f'uplift_{row[\"best_action\"]}'], axis=1\n",
        ")\n",
        "\n",
        "# Only recommend action if expected profit > 0, otherwise \"No Offer\"\n",
        "uplift_data_clean['recommended_action'] = uplift_data_clean.apply(\n",
        "    lambda row: row['best_action'] if row['best_action_profit'] > 0 else 'No Offer',\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\" Recommendations generated!\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n RECOMMENDATION SUMMARY:\")\n",
        "print(\"=\" * 80)\n",
        "recommendation_dist = uplift_data_clean['recommended_action'].value_counts()\n",
        "print(recommendation_dist)\n",
        "print(f\"\\nPercentages:\")\n",
        "print((recommendation_dist / len(uplift_data_clean) * 100).round(2))\n",
        "\n",
        "# Average expected profit by recommended action\n",
        "profit_by_action = uplift_data_clean.groupby('recommended_action')['best_action_profit'].mean()\n",
        "print(f\"\\n Average Expected Profit by Recommended Action:\")\n",
        "print(profit_by_action.round(2))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Recommendation distribution\n",
        "recommendation_dist.plot(kind='bar', ax=axes[0],\n",
        "                        color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#95a5a6'],\n",
        "                        edgecolor='black')\n",
        "axes[0].set_title('Next Best Action Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Recommended Action')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add count labels\n",
        "for i, v in enumerate(recommendation_dist):\n",
        "    axes[0].text(i, v + 500, f'{v:,}\\n({v/len(uplift_data_clean)*100:.1f}%)',\n",
        "                 ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Expected profit distribution\n",
        "uplift_data_clean[uplift_data_clean['best_action_profit'] > 0]['best_action_profit'].hist(\n",
        "    bins=50, ax=axes[1], color='#2ecc71', edgecolor='black', alpha=0.7\n",
        ")\n",
        "axes[1].set_xlabel('Expected Profit ($)', fontsize=12)\n",
        "axes[1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1].set_title('Expected Profit Distribution (Positive Only)', fontsize=12, fontweight='bold')\n",
        "axes[1].axvline(uplift_data_clean['best_action_profit'].mean(), color='red',\n",
        "                linestyle='--', linewidth=2,\n",
        "                label=f'Mean: ${uplift_data_clean[\"best_action_profit\"].mean():.2f}')\n",
        "axes[1].legend()\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Recommendations complete!\")"
      ],
      "metadata": {
        "id": "vxoYPaM-cMMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyze NBA recommendations by customer segment\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" NBA RECOMMENDATIONS BY SEGMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Recommendations by segment\n",
        "segment_recommendations = pd.crosstab(\n",
        "    uplift_data_clean['segment_name'],\n",
        "    uplift_data_clean['recommended_action'],\n",
        "    normalize='index'\n",
        ") * 100\n",
        "\n",
        "print(\"\\n Recommendation Distribution by Segment (%):\")\n",
        "display(segment_recommendations.round(1))\n",
        "\n",
        "# Average expected profit by segment\n",
        "segment_profit = uplift_data_clean.groupby('segment_name').agg({\n",
        "    'best_action_profit': 'mean',\n",
        "    'best_action_uplift': 'mean',\n",
        "    'customer_id': 'count'\n",
        "}).round(2)\n",
        "segment_profit.columns = ['avg_expected_profit', 'avg_uplift', 'count']\n",
        "\n",
        "print(\"\\n Average Expected Profit by Segment:\")\n",
        "display(segment_profit.sort_values('avg_expected_profit', ascending=False))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "segment_recommendations.plot(kind='bar', stacked=True, ax=ax,\n",
        "                            color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#95a5a6'],\n",
        "                            edgecolor='black')\n",
        "ax.set_title('Recommended Actions by Segment (%)', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Segment', fontsize=12)\n",
        "ax.set_ylabel('Percentage (%)', fontsize=12)\n",
        "ax.legend(title='Recommended Action', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Segment analysis complete!\")"
      ],
      "metadata": {
        "id": "u8zEIz7KcVfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate overall business impact of NBA system\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CALCULATING BUSINESS IMPACT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Scenario 1: Random offer assignment (baseline)\n",
        "random_profit_per_customer = uplift_data_clean[profit_columns].mean().mean()\n",
        "\n",
        "# Scenario 2: Always send best performing offer overall\n",
        "best_overall_offer = uplift_data_clean[profit_columns].mean().idxmax().replace('expected_profit_', '')\n",
        "best_overall_profit = uplift_data_clean[f'expected_profit_{best_overall_offer}'].mean()\n",
        "\n",
        "# Scenario 3: NBA system (personalized recommendations)\n",
        "nba_profit = uplift_data_clean['best_action_profit'].mean()\n",
        "\n",
        "# Scenario 4: No offers at all\n",
        "no_offer_profit = 0\n",
        "\n",
        "print(\"\\n BUSINESS IMPACT COMPARISON:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "scenarios = pd.DataFrame({\n",
        "    'Scenario': [\n",
        "        '1. No Offers',\n",
        "        '2. Random Assignment',\n",
        "        f'3. Best Single Offer ({best_overall_offer})',\n",
        "        '4. NBA System (Personalized)'\n",
        "    ],\n",
        "    'Avg Profit per Customer': [\n",
        "        no_offer_profit,\n",
        "        random_profit_per_customer,\n",
        "        best_overall_profit,\n",
        "        nba_profit\n",
        "    ]\n",
        "})\n",
        "\n",
        "scenarios['Total Profit (All Customers)'] = scenarios['Avg Profit per Customer'] * len(uplift_data_clean)\n",
        "scenarios['Lift vs Random (%)'] = ((scenarios['Avg Profit per Customer'] / random_profit_per_customer - 1) * 100).round(1)\n",
        "scenarios['Lift vs Best Single (%)'] = ((scenarios['Avg Profit per Customer'] / best_overall_profit - 1) * 100).round(1)\n",
        "\n",
        "display(scenarios)\n",
        "\n",
        "# Calculate incremental value\n",
        "incremental_vs_random = (nba_profit - random_profit_per_customer) * len(uplift_data_clean)\n",
        "incremental_vs_best = (nba_profit - best_overall_profit) * len(uplift_data_clean)\n",
        "\n",
        "print(f\"\\n INCREMENTAL VALUE:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"NBA vs Random Assignment:\")\n",
        "print(f\"   Incremental profit: ${incremental_vs_random:,.2f}\")\n",
        "print(f\"   Lift: {((nba_profit/random_profit_per_customer - 1)*100):.1f}%\")\n",
        "\n",
        "print(f\"\\nNBA vs Best Single Offer:\")\n",
        "print(f\"   Incremental profit: ${incremental_vs_best:,.2f}\")\n",
        "print(f\"   Lift: {((nba_profit/best_overall_profit - 1)*100):.1f}%\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Profit comparison\n",
        "scenarios_plot = scenarios.set_index('Scenario')['Avg Profit per Customer']\n",
        "scenarios_plot.plot(kind='barh', ax=axes[0],\n",
        "                   color=['#95a5a6', '#e67e22', '#3498db', '#2ecc71'],\n",
        "                   edgecolor='black')\n",
        "axes[0].set_xlabel('Average Profit per Customer ($)', fontsize=12)\n",
        "axes[0].set_title('Profit Comparison Across Strategies', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(scenarios_plot):\n",
        "    axes[0].text(v + 0.01, i, f'${v:.2f}', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Lift comparison\n",
        "lift_data = scenarios.set_index('Scenario')[['Lift vs Random (%)', 'Lift vs Best Single (%)']].iloc[1:]\n",
        "lift_data.plot(kind='bar', ax=axes[1], color=['#e67e22', '#3498db'], edgecolor='black')\n",
        "axes[1].set_ylabel('Lift (%)', fontsize=12)\n",
        "axes[1].set_title('Lift vs Alternative Strategies', fontsize=12, fontweight='bold')\n",
        "axes[1].legend(['vs Random', 'vs Best Single Offer'])\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].axhline(0, color='black', linewidth=1)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Business impact calculated!\")"
      ],
      "metadata": {
        "id": "fp4o2uP_cbnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Identify top customers for each action type\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" TOP CUSTOMERS BY RECOMMENDED ACTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# For each action, show top 10 customers by expected profit\n",
        "for action in ['BOGO', 'Discount', 'Informational']:\n",
        "    action_customers = uplift_data_clean[\n",
        "        uplift_data_clean['recommended_action'] == action\n",
        "    ].nlargest(10, 'best_action_profit')\n",
        "\n",
        "    print(f\"\\n TOP 10 CUSTOMERS FOR {action}:\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if len(action_customers) > 0:\n",
        "        display(action_customers[[\n",
        "            'customer_id',\n",
        "            'segment_name',\n",
        "            'churn_risk_score',\n",
        "            'best_action_uplift',\n",
        "            'best_action_profit',\n",
        "            'rfm_lifetime_frequency',\n",
        "            'rfm_lifetime_monetary_total'\n",
        "        ]].head(10))\n",
        "\n",
        "        print(f\"\\nSummary:\")\n",
        "        print(f\"   Total customers recommended {action}: {(uplift_data_clean['recommended_action']==action).sum():,}\")\n",
        "        print(f\"   Avg expected profit: ${action_customers['best_action_profit'].mean():.2f}\")\n",
        "        print(f\"   Avg churn risk: {action_customers['churn_risk_score'].mean():.3f}\")\n",
        "    else:\n",
        "        print(f\"   No customers recommended for {action}\")\n",
        "\n",
        "print(\"\\n Top customer analysis complete!\")"
      ],
      "metadata": {
        "id": "GxaYz7lhcfPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create final NBA recommendations output\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CREATING NBA RECOMMENDATIONS OUTPUT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Aggregate to customer level (average across all their offer instances)\n",
        "nba_output = uplift_data_clean.groupby('customer_id').agg({\n",
        "    # Demographics\n",
        "    'segment': 'first',\n",
        "    'segment_name': 'first',\n",
        "    'age_clean': 'first',\n",
        "    'income_clean': 'first',\n",
        "    'gender': 'first',\n",
        "\n",
        "    # Churn risk\n",
        "    'churn_risk_score': 'first',\n",
        "    'churn_risk_category': 'first',\n",
        "\n",
        "    # RFM\n",
        "    'rfm_lifetime_frequency': 'first',\n",
        "    'rfm_lifetime_monetary_total': 'first',\n",
        "    'rfm_lifetime_recency_days': 'first',\n",
        "\n",
        "    # NBA recommendations (mode/average)\n",
        "    'recommended_action': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
        "    'best_action_profit': 'mean',\n",
        "    'best_action_uplift': 'mean',\n",
        "\n",
        "    # Individual treatment profits (average)\n",
        "    'expected_profit_BOGO': 'mean',\n",
        "    'expected_profit_Discount': 'mean',\n",
        "    'expected_profit_Informational': 'mean',\n",
        "\n",
        "    # Uplift scores\n",
        "    'uplift_BOGO': 'mean',\n",
        "    'uplift_Discount': 'mean',\n",
        "}).reset_index()\n",
        "\n",
        "print(f\"\\n NBA Output Dataset:\")\n",
        "print(f\"   Customers: {len(nba_output):,}\")\n",
        "print(f\"   Columns: {nba_output.shape[1]}\")\n",
        "\n",
        "# Add top 3 reasons (top features from uplift model)\n",
        "# For simplicity, we'll use feature importance from earlier\n",
        "print(\"\\n Adding top predictive features...\")\n",
        "\n",
        "# Get top 3 features for each customer based on their values\n",
        "# This is simplified - in production you'd use SHAP for individual explanations\n",
        "top_rfm_features = ['rfm_lifetime_recency_days', 'rfm_lifetime_frequency', 'rfm_lifetime_monetary_total']\n",
        "nba_output['top_reason_1'] = 'Recency: ' + nba_output['rfm_lifetime_recency_days'].round().astype(str) + ' days'\n",
        "nba_output['top_reason_2'] = 'Frequency: ' + nba_output['rfm_lifetime_frequency'].round().astype(str) + ' txns'\n",
        "nba_output['top_reason_3'] = 'Total Spend: $' + nba_output['rfm_lifetime_monetary_total'].round().astype(str)\n",
        "\n",
        "print(\" NBA output created!\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\n Sample NBA Recommendations:\")\n",
        "display(nba_output[[\n",
        "    'customer_id',\n",
        "    'segment_name',\n",
        "    'churn_risk_category',\n",
        "    'recommended_action',\n",
        "    'best_action_profit',\n",
        "    'best_action_uplift'\n",
        "]].head(20))\n",
        "\n",
        "# Summary by segment and risk\n",
        "print(\"\\n Recommendations by Segment and Risk:\")\n",
        "summary = nba_output.groupby(['segment_name', 'churn_risk_category', 'recommended_action']).size().unstack(fill_value=0)\n",
        "display(summary)\n",
        "\n",
        "print(\"\\n NBA recommendations ready!\")"
      ],
      "metadata": {
        "id": "iVlx27q7ch0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Evaluate uplift model performance using Qini curves and AUUC\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" UPLIFT MODEL EVALUATION - QINI CURVES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        "Qini Curve measures how well the model ranks customers by uplift.\n",
        "It shows the cumulative gain achieved by targeting customers in order of predicted uplift.\n",
        "\n",
        "AUUC (Area Under Uplift Curve) is the summary metric - higher is better.\n",
        "\"\"\")\n",
        "\n",
        "def calculate_qini_curve(y_true, treatment, uplift_score, n_bins=10):\n",
        "    \"\"\"\n",
        "    Calculate Qini curve for uplift evaluation\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : array\n",
        "        Actual outcomes (conversions)\n",
        "    treatment : array\n",
        "        Treatment assignment (1 for treatment, 0 for control)\n",
        "    uplift_score : array\n",
        "        Predicted uplift scores\n",
        "    n_bins : int\n",
        "        Number of bins for the curve\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    x_values, y_values for plotting\n",
        "    \"\"\"\n",
        "    # Sort by uplift score (descending)\n",
        "    sorted_indices = np.argsort(-uplift_score)\n",
        "    y_sorted = y_true[sorted_indices]\n",
        "    treatment_sorted = treatment[sorted_indices]\n",
        "\n",
        "    # Calculate cumulative gains\n",
        "    n_total = len(y_sorted)\n",
        "    bin_size = n_total // n_bins\n",
        "\n",
        "    x_values = []\n",
        "    y_values = []\n",
        "\n",
        "    for i in range(n_bins + 1):\n",
        "        end_idx = min(i * bin_size, n_total)\n",
        "        if end_idx == 0:\n",
        "            x_values.append(0)\n",
        "            y_values.append(0)\n",
        "            continue\n",
        "\n",
        "        # Get data up to this point\n",
        "        y_bin = y_sorted[:end_idx]\n",
        "        t_bin = treatment_sorted[:end_idx]\n",
        "\n",
        "        # Calculate gains for treatment and control groups\n",
        "        n_treatment = t_bin.sum()\n",
        "        n_control = len(t_bin) - n_treatment\n",
        "\n",
        "        if n_treatment > 0 and n_control > 0:\n",
        "            # Conversions in treatment and control\n",
        "            conv_treatment = y_bin[t_bin == 1].sum()\n",
        "            conv_control = y_bin[t_bin == 0].sum()\n",
        "\n",
        "            # Conversion rates\n",
        "            rate_treatment = conv_treatment / n_treatment\n",
        "            rate_control = conv_control / n_control\n",
        "\n",
        "            # Qini value = incremental conversions\n",
        "            qini = (rate_treatment - rate_control) * end_idx\n",
        "        else:\n",
        "            qini = 0\n",
        "\n",
        "        x_values.append(end_idx / n_total)\n",
        "        y_values.append(qini)\n",
        "\n",
        "    return np.array(x_values), np.array(y_values)\n",
        "\n",
        "# Evaluate uplift for each treatment vs control\n",
        "print(\"\\n Calculating Qini curves...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "colors_map = {'BOGO': '#FF6B6B', 'Discount': '#4ECDC4'}\n",
        "\n",
        "for idx, treatment in enumerate(['BOGO', 'Discount']):\n",
        "    print(f\"\\n Evaluating {treatment} vs {control_group}...\")\n",
        "\n",
        "    # Get data for this treatment and control\n",
        "    treatment_mask = uplift_data_clean['offer_type_simple'] == treatment\n",
        "    control_mask = uplift_data_clean['offer_type_simple'] == control_group\n",
        "    combined_mask = treatment_mask | control_mask\n",
        "\n",
        "    y_combined = uplift_data_clean.loc[combined_mask, 'conv_14d_conversion'].values\n",
        "    t_combined = treatment_mask[combined_mask].astype(int).values\n",
        "    uplift_combined = uplift_data_clean.loc[combined_mask, f'uplift_{treatment}'].values\n",
        "\n",
        "    # Calculate Qini curve\n",
        "    x_qini, y_qini = calculate_qini_curve(y_combined, t_combined, uplift_combined, n_bins=20)\n",
        "\n",
        "    # Calculate AUUC (Area Under Uplift Curve) using trapezoidal rule\n",
        "    auuc = np.trapz(y_qini, x_qini)\n",
        "\n",
        "    print(f\"   AUUC: {auuc:.4f}\")\n",
        "\n",
        "    # Plot\n",
        "    axes[0].plot(x_qini, y_qini, linewidth=2, marker='o', markersize=4,\n",
        "                label=f'{treatment} (AUUC={auuc:.3f})', color=colors_map[treatment])\n",
        "\n",
        "# Random model baseline\n",
        "axes[0].plot([0, 1], [0, 0], 'k--', linewidth=2, label='Random Model')\n",
        "axes[0].set_xlabel('Fraction of Population Targeted', fontsize=12)\n",
        "axes[0].set_ylabel('Cumulative Incremental Gain', fontsize=12)\n",
        "axes[0].set_title('Qini Curves - Uplift Model Performance', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Uplift distribution by bins\n",
        "for treatment in ['BOGO', 'Discount']:\n",
        "    uplift_scores_sorted = np.sort(uplift_data_clean[f'uplift_{treatment}'].values)[::-1]\n",
        "    n = len(uplift_scores_sorted)\n",
        "    bins = np.array_split(uplift_scores_sorted, 10)\n",
        "    bin_means = [b.mean() for b in bins]\n",
        "\n",
        "    axes[1].plot(range(1, 11), bin_means, linewidth=2, marker='o', markersize=6,\n",
        "                label=treatment, color=colors_map[treatment])\n",
        "\n",
        "axes[1].set_xlabel('Decile (1=Highest Predicted Uplift)', fontsize=12)\n",
        "axes[1].set_ylabel('Average Uplift Score', fontsize=12)\n",
        "axes[1].set_title('Average Uplift by Decile', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axhline(0, color='red', linestyle='--', linewidth=1, label='No Effect')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Qini curve evaluation complete!\")"
      ],
      "metadata": {
        "id": "8o-aQDXIcj9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate profit gained by targeting top K% of customers\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" PROFIT @ TOP-K ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n How much profit do we gain by targeting the top K% of customers?\")\n",
        "\n",
        "# Sort customers by expected profit\n",
        "nba_output_sorted = nba_output.sort_values('best_action_profit', ascending=False)\n",
        "\n",
        "# Calculate cumulative profit at different thresholds\n",
        "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1.0]  # Top 10%, 20%, etc.\n",
        "\n",
        "profit_at_k = []\n",
        "\n",
        "for k in thresholds:\n",
        "    n_customers = int(len(nba_output_sorted) * k)\n",
        "    top_k = nba_output_sorted.head(n_customers)\n",
        "\n",
        "    total_profit = top_k['best_action_profit'].sum()\n",
        "    avg_profit = top_k['best_action_profit'].mean()\n",
        "\n",
        "    profit_at_k.append({\n",
        "        'Top K%': f'{k*100:.0f}%',\n",
        "        'N Customers': n_customers,\n",
        "        'Total Profit': total_profit,\n",
        "        'Avg Profit': avg_profit,\n",
        "        'Profit per Customer (All)': total_profit / len(nba_output)\n",
        "    })\n",
        "\n",
        "profit_at_k_df = pd.DataFrame(profit_at_k)\n",
        "\n",
        "print(\"\\n Profit@Top-K Results:\")\n",
        "display(profit_at_k_df)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Cumulative profit curve\n",
        "x_pct = [p['Top K%'] for p in profit_at_k]\n",
        "y_total = [p['Total Profit'] for p in profit_at_k]\n",
        "\n",
        "axes[0].plot(range(len(x_pct)), y_total, linewidth=3, marker='o', markersize=8, color='#2ecc71')\n",
        "axes[0].set_xticks(range(len(x_pct)))\n",
        "axes[0].set_xticklabels(x_pct)\n",
        "axes[0].set_xlabel('Top K% of Customers (by Predicted Profit)', fontsize=12)\n",
        "axes[0].set_ylabel('Total Profit ($)', fontsize=12)\n",
        "axes[0].set_title('Cumulative Profit by Targeting Top K%', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(y_total):\n",
        "    axes[0].text(i, v + max(y_total)*0.02, f'${v:,.0f}', ha='center', fontsize=9)\n",
        "\n",
        "# Average profit per customer\n",
        "y_avg = [p['Avg Profit'] for p in profit_at_k]\n",
        "\n",
        "axes[1].bar(range(len(x_pct)), y_avg, color='#3498db', edgecolor='black')\n",
        "axes[1].set_xticks(range(len(x_pct)))\n",
        "axes[1].set_xticklabels(x_pct)\n",
        "axes[1].set_xlabel('Top K% of Customers', fontsize=12)\n",
        "axes[1].set_ylabel('Average Profit per Customer ($)', fontsize=12)\n",
        "axes[1].set_title('Average Profit in Top K%', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n INTERPRETATION:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"â€¢ By targeting top 10% of customers, we get ${profit_at_k[0]['Total Profit']:,.2f} in profit\")\n",
        "print(f\"â€¢ By targeting top 50% of customers, we get ${profit_at_k[4]['Total Profit']:,.2f} in profit\")\n",
        "print(f\"â€¢ Total profit from all customers: ${profit_at_k[-1]['Total Profit']:,.2f}\")\n",
        "\n",
        "print(\"\\n Profit@Top-K analysis complete!\")"
      ],
      "metadata": {
        "id": "L4eShJXOcwAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Save all NBA models and outputs\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" SAVING NBA SYSTEM\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save uplift models\n",
        "print(\"\\n Saving uplift models...\")\n",
        "for treatment in treatments:\n",
        "    if treatment != control_group:\n",
        "        with open(f'cleaned_data/uplift_model_{treatment}.pkl', 'wb') as f:\n",
        "            pickle.dump(treatment_models[treatment], f)\n",
        "        print(f\" Saved: uplift_model_{treatment}.pkl\")\n",
        "\n",
        "with open('cleaned_data/uplift_model_control.pkl', 'wb') as f:\n",
        "    pickle.dump(model_control, f)\n",
        "print(f\" Saved: uplift_model_control.pkl\")\n",
        "\n",
        "# Save scaler\n",
        "with open('cleaned_data/scaler_uplift.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_uplift, f)\n",
        "print(\" Saved: scaler_uplift.pkl\")\n",
        "\n",
        "# Save feature list\n",
        "with open('cleaned_data/uplift_features.pkl', 'wb') as f:\n",
        "    pickle.dump(uplift_features, f)\n",
        "print(\" Saved: uplift_features.pkl\")\n",
        "\n",
        "# Save offer costs\n",
        "with open('cleaned_data/offer_costs.pkl', 'wb') as f:\n",
        "    pickle.dump(OFFER_COSTS, f)\n",
        "print(\" Saved: offer_costs.pkl\")\n",
        "\n",
        "# Save NBA outputs\n",
        "nba_output.to_csv('cleaned_data/nba_recommendations.csv', index=False)\n",
        "print(\" Saved: nba_recommendations.csv\")\n",
        "\n",
        "nba_output.to_pickle('cleaned_data/nba_recommendations.pkl')\n",
        "print(\"Saved: nba_recommendations.pkl\")\n",
        "\n",
        "# Save full uplift data\n",
        "uplift_data_clean.to_csv('cleaned_data/uplift_data_complete.csv', index=False)\n",
        "print(\" Saved: uplift_data_complete.csv\")\n",
        "\n",
        "uplift_data_clean.to_pickle('cleaned_data/uplift_data_complete.pkl')\n",
        "print(\" Saved: uplift_data_complete.pkl\")\n",
        "\n",
        "print(\"\\n All NBA system components saved!\")\n",
        "print(f\"   Location: cleaned_data/\")"
      ],
      "metadata": {
        "id": "n4cpfHT0cyur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create a simple API function for getting recommendations\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" CREATING NBA API FUNCTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def get_nba_recommendation(customer_id, nba_output_df):\n",
        "    \"\"\"\n",
        "    Get Next Best Action recommendation for a customer\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    customer_id : str\n",
        "        Customer ID\n",
        "    nba_output_df : DataFrame\n",
        "        NBA recommendations dataframe\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Recommendation details\n",
        "    \"\"\"\n",
        "\n",
        "    if customer_id not in nba_output_df['customer_id'].values:\n",
        "        return {\n",
        "            'error': 'Customer not found',\n",
        "            'customer_id': customer_id\n",
        "        }\n",
        "\n",
        "    customer = nba_output_df[nba_output_df['customer_id'] == customer_id].iloc[0]\n",
        "\n",
        "    # Helper function to convert numpy types to Python types\n",
        "    def to_python_type(val):\n",
        "        if pd.isna(val):\n",
        "            return None\n",
        "        if isinstance(val, (np.integer, np.int64, np.int32)):\n",
        "            return int(val)\n",
        "        if isinstance(val, (np.floating, np.float64, np.float32)):\n",
        "            return float(val)\n",
        "        return val\n",
        "\n",
        "    # Build alternative actions dict - only include columns that exist\n",
        "    alternative_actions = {}\n",
        "\n",
        "    for treatment in ['BOGO', 'Discount', 'Informational']:\n",
        "        uplift_col = f'uplift_{treatment}'\n",
        "        profit_col = f'expected_profit_{treatment}'\n",
        "\n",
        "        if uplift_col in customer.index and profit_col in customer.index:\n",
        "            alternative_actions[treatment] = {\n",
        "                'uplift': round(float(customer[uplift_col]), 4),\n",
        "                'profit': round(float(customer[profit_col]), 2)\n",
        "            }\n",
        "        else:\n",
        "            # If columns don't exist, set to 0 (baseline/control)\n",
        "            alternative_actions[treatment] = {\n",
        "                'uplift': 0.0,\n",
        "                'profit': 0.0\n",
        "            }\n",
        "\n",
        "    recommendation = {\n",
        "        'customer_id': str(customer_id),\n",
        "        'segment': str(customer['segment_name']),\n",
        "        'churn_risk': str(customer['churn_risk_category']),\n",
        "        'churn_risk_score': round(float(customer['churn_risk_score']), 3),\n",
        "        'recommended_action': str(customer['recommended_action']),\n",
        "        'expected_uplift': round(float(customer['best_action_uplift']), 4),\n",
        "        'expected_profit': round(float(customer['best_action_profit']), 2),\n",
        "        'alternative_actions': alternative_actions,\n",
        "        'customer_profile': {\n",
        "            'lifetime_transactions': int(customer['rfm_lifetime_frequency']),\n",
        "            'lifetime_spend': round(float(customer['rfm_lifetime_monetary_total']), 2),\n",
        "            'days_since_last_purchase': int(customer['rfm_lifetime_recency_days']),\n",
        "            'age': int(customer['age_clean']) if not pd.isna(customer['age_clean']) else None,\n",
        "            'income': int(customer['income_clean']) if not pd.isna(customer['income_clean']) else None,\n",
        "        },\n",
        "        'top_reasons': [\n",
        "            str(customer['top_reason_1']),\n",
        "            str(customer['top_reason_2']),\n",
        "            str(customer['top_reason_3'])\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return recommendation\n",
        "\n",
        "# Test the API function\n",
        "print(\"\\n Testing NBA API function...\")\n",
        "sample_customer_id = nba_output['customer_id'].iloc[0]\n",
        "sample_recommendation = get_nba_recommendation(sample_customer_id, nba_output)\n",
        "\n",
        "print(f\"\\n Sample Recommendation for Customer: {sample_customer_id}\")\n",
        "print(\"=\" * 80)\n",
        "import json\n",
        "print(json.dumps(sample_recommendation, indent=2))\n",
        "\n",
        "print(\"\\n NBA API function created and tested!\")\n",
        "\n",
        "# Test a few more customers\n",
        "print(\"\\n Testing with 3 different customers:\")\n",
        "for i in range(3):\n",
        "    cust_id = nba_output['customer_id'].iloc[i]\n",
        "    rec = get_nba_recommendation(cust_id, nba_output)\n",
        "    print(f\"\\n{i+1}. Customer {cust_id}:\")\n",
        "    print(f\"   Segment: {rec['segment']}\")\n",
        "    print(f\"   Churn Risk: {rec['churn_risk']}\")\n",
        "    print(f\"   Recommended Action: {rec['recommended_action']}\")\n",
        "    print(f\"   Expected Profit: ${rec['expected_profit']:.2f}\")\n",
        "\n",
        "print(\"\\n NBA API function ready for deployment!\")"
      ],
      "metadata": {
        "id": "ge1sve1Gc4mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FuaVfuUNc_a9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}